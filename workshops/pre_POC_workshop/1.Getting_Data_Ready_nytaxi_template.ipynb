{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data input for Amazon Forecast\n",
    "\n",
    "Forecasting is used in a variety of applications and business use cases: For example, retailers need to forecast the sales of their products to decide how much stock they need by location, Manufacturers need to estimate the number of parts required at their factories to optimize their supply chain, Businesses need to estimate their flexible workforce needs, Utilities need to forecast electricity consumption needs in order to attain an efficient energy network, and enterprises need to estimate their cloud infrastructure needs.\n",
    "<img src=\"https://amazon-forecast-samples.s3-us-west-2.amazonaws.com/common/images/forecast_overview.png\" width=\"98%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook Overview\n",
    "\n",
    "This notebook provides a template of typical steps required to prepare data input for Amazon Forecast. So, this is part of \"Upload your data to Amazon Forecast\" in the diagram above.  Your data may need additional Prepare steps and/or you may not need all the steps in this notebook.<br>\n",
    "\n",
    "You will need lots of customization in the RTS section.  Otherwise, look for following comment string to indicate places you will need to customize this notebook for your data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####\n",
    "# EDIT THIS FOR YOUR DATA\n",
    "####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Step 1 Read data</b>. Modify this section to read your data.\n",
    "\n",
    "<b>Step 2 Make forecast choices</b>. You need to make some decisions.  How many time units in future do you want to make forecasts?  E.g. if time unit is Hour, then if you want to forecast out 1 week that would be 24*7 = 168 hours.  You'll also be asked if you want integer or float target_values.  And if 0's really mean 0 or are they missing data?\n",
    "\n",
    "<b>Step 10 Aggregate based on the time granularity of your data.</b> Possible aggregations are minute, hour, day, week, month.  See <a href=\"https://docs.aws.amazon.com/forecast/latest/dg/howitworks-datasets-groups.html#howitworks-data-alignment\" target=\"_blank\">documenatation. </a>\n",
    "\n",
    "<b>Step 12 Visualization time series and validate the time granularity you chose makes sense.</b>  Too low-granularity means the time series looks like white noise and the data could benefit by aggregation at higher-time-level.  For example, you might start to notice time series cyclical patterns when aggregating historical sales by-hour instead of of by-minute.  Alternate between Step 7 Visualization and Step 5 Aggregation until you are happy with the chosen time-grandularity for your forecasts.\n",
    "\n",
    "<b>Steps 10 (Aggregate) and 13 (Split train) include a set of common checks on your data for forecasting.</b> One common mistake is customers think they should \"try Amazon Forecast with a small sample of data\".  If the majority of your time series have fewer than 300 data points, Amazon Forecast will fail, because Amazon Forecast is designed for deep-learning AI forecasting algorithms, which require many data points (typically 1000+) per time series.  Small data is better off using traditional, open-source forecast methods such as ETS (available in Excel), ARIMA, or Prophet.  Small data is not a good fit for Amazon Forecast.  If you really want to try Amazon Forecast, but your data fails the Error Checks consider: \n",
    "\n",
    "<ul>\n",
    "    <li>Can you get more data such that each time series will have longer history with more data points?  If so, return to Step 1.</li>\n",
    "    <li>Can you combine items into fewer item_ids such that each time series will have longer history with more data points?  If so, return to Step 1.</li>\n",
    "    <li>Can you reduce forecast dimensions, such as use item_id only and drop location_id?  If so, return to Step 5.</li>\n",
    "    <li>Can you drop to a lower time-frequency without your data turning into white noise?  Check your data again using Step 10 Aggregate and Step 12 Visualize. </li>\n",
    "</ul>\n",
    " \n",
    "It may be that you find your data is not a good fit for Amazon Forecast.  In that case, it's better that you discover this early.\n",
    "\n",
    "<b>In steps 14-20, we will save headerless Target Time Series (TTS), Item metadata (IM), Related Time Series (RTS) to S3</b> so we can trigger Amazon Forecast automation, see\n",
    "    <a href=\"https://github.com/aws-samples/amazon-forecast-samples/tree/master/workshops/CloudFormationAutomation\" target=\"_blank\">Workshop Instructions Install Automation and Demo</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:  bucket steps so list doesn't look so long! <br>\n",
    "TODO:  Add data normalization step\n",
    "\n",
    "# Table of Contents \n",
    "* Step 0: [Set up and install libraries](#setup)\n",
    "* Step 1: [Read data](#read)\n",
    "* Step 2: [Correct dtypes](#fix_dtypes)\n",
    "* Step 3: [Make forecast choices](#choices)\n",
    "* Step 4: [Drop null item_ids](#drop_null_items)\n",
    "* Step 5: [Drop null timestamps](#drop_null_times) \n",
    "* Step 7: [Inspect and treat extremes](#treat_extremes) \n",
    "* Step 8: [Optional - Round negative targets up to 0](#round_negatives) \n",
    "* Step 9: [Optional - Convert negative targets to_nan](#negatives_to_nan) \n",
    "* Step 10: [Aggregate at chosen frequency](#groupby_frequency) \n",
    "* Step 11: [Typical Retail scenario: Find top-moving items](#top_moving_items)\n",
    "* Step 12: [Visualize time series](#visualize) \n",
    "* Step 13: [Split train/test data](#split_train_test)\n",
    "* Step 14: [Prepare and save Target Time Series](#TTS) \n",
    "* Step 15: [Remove time series with no target values at all](#TTS_remove_all0)\n",
    "* Step 16: [Remove time series with end of life](#TTS_remove_end_of_life)\n",
    "* Step 17: [Remove time series with fewer than 5 data points](#TTS_remove_too_few_data_points)\n",
    "* Step 18: [Optional - Assemble and save TTS_sparse, TTS_dense](#TTS-dense_sparse)\n",
    "* Step 19: [Optional - Assemble and save TTS_top, TTS_slow](#TTS_top)\n",
    "* Step 20: [Assemble and save RTS (if any)](#RTS)\n",
    "* Step 21: [Classify time series](#Classify)\n",
    "* Step 22: [Optional - Assemble and save TTS_regular, TTS_erratic, TTS_intermittent, TTS_lumpy](#TTS_classes)\n",
    "* Step 23: [Assemble and save metadata (if any)](#IM) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data used in these notebooks: NYC Taxi trips open data\n",
    "\n",
    "Given hourly historical taxi trips data for NYC, your task is to predict #pickups in next 7 days, per hour and per pickup zone.  <br>\n",
    "\n",
    "<ul>\n",
    "<li>Original data source:  <a href=\"https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page\" target=\"_blank\"> https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page</a> </li>\n",
    "<li>AWS-hosted public source:  <a href=\"https://registry.opendata.aws/nyc-tlc-trip-records-pds/\" target=\"_blank\">https://registry.opendata.aws/nyc-tlc-trip-records-pds/ </a> </li>\n",
    "<li>AWS managed weather data ingestion as a service that is bundled with Amazon Forecast, aggregated by location and by hour.  Initially only for USA and Europe, but depending on demand, possibly in the future for other global regions. </li>\n",
    "<li>Data used:  Yellow taxis dates: 2018-12 through 2020-02 to avoid COVID effects </li>\n",
    "</ul>\n",
    "\n",
    " \n",
    "### Features and cleaning\n",
    "Note: ~5GB Raw Data has already been cleaned and joined using AWS Glue (tutorials to be created in future). \n",
    "<ul>\n",
    "    <li>Join shape files Latitude, Longitude</li>\n",
    "    <li>Add Trip duration in minutes</li>\n",
    "    <li>Drop negative trip distances, 0 fares, 0 passengers, less than 1min trip durations </li>\n",
    "    <li>Drop 2 unknown zones ['264', '265']\n",
    "    </ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0:  Set up and install libraries <a class=\"anchor\" id=\"setup\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import standard open libraries\n",
    "import os\n",
    "import sys\n",
    "import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "print('pandas: {}'.format(pd.__version__))\n",
    "# display all columns wide\n",
    "pd.set_option('display.max_columns', None)\n",
    "# display horizontal scrollbar for wide columns\n",
    "pd.set_option('display.width', 5000)\n",
    "pd.set_option('display.max_colwidth', 5000)\n",
    "# display all rows\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "#turn off scientific notation\n",
    "pd.set_option('display.float_format', lambda x: '%.5f' % x)\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "print('numpy: {}'.format(np.__version__))\n",
    "import matplotlib as mpl\n",
    "print('matplotlib: {}'.format(mpl.__version__))\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "import seaborn as sns\n",
    "print('seaborn: {}'.format(sns.__version__))\n",
    "# choose colorblind\n",
    "color_pal = sns.color_palette(\"colorblind\", 6).as_hex()\n",
    "colorblind6 = ','.join(color_pal).split(\",\")\n",
    "\n",
    "# AWS libraries and initialization\n",
    "import boto3\n",
    "# importing forecast notebook utility from notebooks/common directory\n",
    "sys.path.insert( 0, os.path.abspath(\"../../notebooks/common\") )\n",
    "import util\n",
    "import util.fcst_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########\n",
    "# Functions to classify items as \"top movers\" or not\n",
    "# TODO: generaize to handle any number of forecast dimensions, not just location\n",
    "#########\n",
    "\n",
    "def get_time_min_max(the_df, item_id_col, timestamp_col, location_id_col=None):\n",
    "    \"\"\"Calculate min timestamp, max timestamp per item and/or per item-location time series  \n",
    "       Inputs: pandas dataframe with columns: timestamp, target_value, item_id, location_id (optional)\n",
    "       Outputs: pandas dataframe with 2 extra columns \"min_time\" and \"max_time\"\n",
    "    \"\"\"\n",
    "    df = the_df.copy()\n",
    "    \n",
    "    if location_id_col is None:\n",
    "        # get max\n",
    "        max_time_df = \\\n",
    "            df.groupby([item_id_col], as_index=False).max()[[item_id_col, timestamp_col]]\n",
    "        max_time_df.columns = [item_id_col, 'max_time']\n",
    "        # get min\n",
    "        min_time_df = df.groupby([item_id_col], as_index=False).min()[[item_id_col, timestamp_col]]\n",
    "        min_time_df.columns = [item_id_col, 'min_time']\n",
    "        # merge 2 extra columns per item grouping: max and min\n",
    "        df = df.merge(right=max_time_df, on=item_id_col)\n",
    "        df = df.merge(right=min_time_df, on=item_id_col)\n",
    "        \n",
    "    else:\n",
    "        # get max\n",
    "        max_time_df = \\\n",
    "            df.groupby([item_id_col, location_id_col], as_index=False).max()[[item_id_col, location_id_col, timestamp_col]]\n",
    "        max_time_df.columns = [item_id_col, location_id_col, 'max_time']\n",
    "        # get min\n",
    "        min_time_df = df.groupby([item_id_col, location_id_col], as_index=False).min()[[item_id_col, location_id_col, timestamp_col]]\n",
    "        min_time_df.columns = [item_id_col, location_id_col, 'min_time']\n",
    "        # merge 2 extra columns per item grouping: max and min\n",
    "        df = df.merge(right=max_time_df, on=[item_id_col, location_id_col])\n",
    "        df = df.merge(right=min_time_df, on=[item_id_col, location_id_col])\n",
    "        \n",
    "    return df\n",
    "\n",
    "\n",
    "def get_velocity_per_item(the_df, timestamp_col, target_value_col, item_id_col, location_id_col=None):\n",
    "    \"\"\"Calculate velocity as target_demand per time unit per time series\n",
    "       Inputs: pandas dataframe with columns: timestamp, target_value, item_id, location_id (optional)\n",
    "       Outputs: pandas dataframe with extra \"velocity\" column\n",
    "    \"\"\"\n",
    "    df = the_df.copy()\n",
    "    df[timestamp_col] = pd.to_datetime(df[timestamp_col], format='%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "    # append 2 extra columns per time seres: min_time, max_time\n",
    "    if location_id_col == None:\n",
    "        df = get_time_min_max(the_df, item_id_col, timestamp_col)\n",
    "    else:\n",
    "        df = get_time_min_max(the_df, item_id_col, timestamp_col, location_id_col)\n",
    "        \n",
    "#     print (df.sample(10))\n",
    "    \n",
    "    # calculate time span per time seres\n",
    "    df['time_span'] = df['max_time'] - df['min_time']\n",
    "    df['time_span'] = df['time_span'].apply(lambda x: x.seconds / 3600 + 1) # add 1 to include start datetime and end datetime\n",
    "    \n",
    "    # calculate average item demand per time unit\n",
    "    if location_id_col is None:\n",
    "        df = df.groupby([item_id_col], as_index=False).agg({'time_span':'mean', target_value_col:'sum'})\n",
    "    else:\n",
    "        df = df.groupby([item_id_col, location_id_col], as_index=False).agg({'time_span':'mean', target_value_col:'sum'})\n",
    "    df['velocity'] = df[target_value_col] / df['time_span']\n",
    "    \n",
    "    return df\n",
    "\n",
    "def get_top_moving_items(the_df\n",
    "                           , timestamp_col\n",
    "                           , target_value_col\n",
    "                           , item_id_col\n",
    "                           , location_id_col=None):\n",
    "    \"\"\"Calculate mean velocity over all items as \"criteria\".\n",
    "       Where velocity is demand per time unit per time series\n",
    "       Assign each item into category \"top\" or not depending on whether its velocity > criteria.\n",
    "       Return 4 dataframes:  top_moving and slow_moving items; top_moving and slow_moving time series\n",
    "    \"\"\"\n",
    "    \n",
    "    # calculate item-level velocity\n",
    "    df_items_velocity = the_df.copy().reset_index(drop=True)\n",
    "    df_items_velocity = get_velocity_per_item(df_items_velocity, timestamp_col\n",
    "                                           , target_value_col, item_id_col)\n",
    "    # calculate ts-level velocity\n",
    "    if location_id_col is not None:\n",
    "        df_ts_velocity = the_df.copy().reset_index(drop=True)\n",
    "        df_ts_velocity = get_velocity_per_item(df_ts_velocity, timestamp_col\n",
    "                                               , target_value_col, item_id_col, location_id_col)\n",
    "    else:\n",
    "        print(\"No location found\")\n",
    "        df_ts_velocity = pd.DataFrame()\n",
    "    \n",
    "    #########\n",
    "    # define cut=off criteria for \"top-moving\" vs \"slow-moving\" time series\n",
    "    if location_id_col is None:\n",
    "        criteria = df_items_velocity['velocity'].mean()\n",
    "    else:\n",
    "        criteria = df_ts_velocity['velocity'].mean()\n",
    "        # define top-moving if velocity > criteria\n",
    "        df_ts_velocity['top_moving'] = df_ts_velocity['velocity'] > criteria\n",
    "        print('average velocity of all items:', criteria)\n",
    "    #########\n",
    "    \n",
    "    # define top-moving if velocity > criteria\n",
    "    df_items_velocity['top_moving'] = df_items_velocity['velocity'] > criteria\n",
    "    \n",
    "    return df_items_velocity, df_ts_velocity\n",
    "\n",
    "\n",
    "#########\n",
    "# Function to group time series by different time frequency\n",
    "#########\n",
    "def aggregate_time_series(theDF, agg_freq, timestamp_col, item_col, target_col, agg_dict, already_grouped):\n",
    "    \"\"\"Function that re-groups pandas dataframe by time frequency. Expects a timestamp and item_id column\n",
    "        as time series dimensions.  If additional columns present, you need to put them in the \n",
    "        agg_dict to specify grouping function per additional column.\n",
    "       Input: \n",
    "           theDF = pandas dataframe\n",
    "           timestamp_col, item_col, target_col = expected column in theDF\n",
    "           agg_freq = time freq for grouping, can be \"Y\", \"M\", \"W\", \"D\", \"H\", \"M\"\n",
    "           agg_dict = dictionary of non-key columns w/each column's desired agg fn\n",
    "                      e.g. agg_dict = {\"Qty\":\"sum\", \"location_id\":\"first\", ...}\n",
    "           already_grouped = True if theDF timestamp is already at the desired aggregation level\n",
    "                               ; False otherwise\n",
    "    \"\"\"\n",
    "    print(f\"agg_freq=''{agg_freq}'\")\n",
    "    g = theDF.copy()\n",
    "    \n",
    "    if not already_grouped:\n",
    "        # aggregate by agg_freq\n",
    "        g = g.groupby([pd.Grouper(key=timestamp_col, freq=agg_freq), item_col]).agg(agg_dict)\n",
    "        g.drop_duplicates(inplace=True)\n",
    "\n",
    "        g.reset_index(inplace=True)\n",
    "        print(f\"grouped shape = {g.shape}, original shape = {theDF.shape}\")\n",
    "        display(g.sample(5))\n",
    "    else:\n",
    "        print(\"already grouped\")\n",
    "\n",
    "    # INSPECT DISTRIBUTION OF TIME SERIES LENGTH\n",
    "    # In chart below, you want bulk of distribution to sit after 300-mark on x-axis.\n",
    "    # If bulk of distribution is bunched closer to 0, choose smaller time granularity or aggregate item-level only\n",
    "\n",
    "    # num data points per item\n",
    "    sparsity = g.copy()\n",
    "    sparsity = sparsity.groupby([item_col]).count()\n",
    "    sparsity[[target_col]].hist(bins=32);\n",
    "    plt.title(\"Count number data points per item\");\n",
    "    plt.suptitle(\"In chart below, you want bulk of distribution to sit after 300-mark on x-axis.\")\n",
    "    \n",
    "    dist = sparsity[[target_col]].describe()\n",
    "    print(dist)\n",
    "    \n",
    "    # INSPECT DISTRIBUTION OF TARGET_VALUES\n",
    "    dist = g[target_col].describe()\n",
    "\n",
    "    # In chart below, you want to see the shape of a distribution (counts usually negative-binomial)\n",
    "    # You do not want to see a randomly uniform shape\n",
    "\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    fig.suptitle(\"In chart below, you want to see a distribution, usually negative-binomial or gaussian.\", y=1.08)\n",
    "    # plot distribution of target_value itself\n",
    "    sparsity = g.copy()\n",
    "    sparsity[[target_col]].hist(ax=axs[0])\n",
    "    axs[0].set_title(f\"Distribution of {target_col} at '{agg_freq}' level\", y=1.08);\n",
    "\n",
    "    # zoom-in plot distribution of target_value itself\n",
    "    zoomed = g.copy()\n",
    "    zoomed = zoomed.loc[(zoomed[target_col]< 2*dist['75%']), :].copy()\n",
    "    zoomed[[target_col]].hist(bins=32, ax=axs[1])\n",
    "    axs[1].set_title(f\"Distribution of {target_col} up to 75% percentile\", y=1.08);\n",
    "\n",
    "    return g\n",
    "\n",
    "\n",
    "#########\n",
    "# Function to classify time series\n",
    "#########\n",
    "\n",
    "def calc_ADI(g, time_col, item_col, target_col):\n",
    "    \"\"\"    \n",
    "    Calculates class of time series\n",
    "    Input: g = grouped pandas dataframe already grouped as single time series at a time\n",
    "    Ouptu: pandas dataframe with 2 extra columns 'ADI' and 'ts_type'\n",
    "    Rules:\n",
    "    1. Regular demand (ADI < 1.32 and CV² < 0.49). \n",
    "    2. Intermittent demand (ADI >= 1.32 and CV² < 0.49). \n",
    "    3. Erratic demand (ADI < 1.32 and CV² >= 0.49). \n",
    "    4. Lumpy demand (ADI >= 1.32 and CV² >= 0.49). Unforecastable.\n",
    "    \"\"\"\n",
    "    df = g.copy()\n",
    "    \n",
    "    # calculate ADI\n",
    "    num_periods = df[time_col].nunique()\n",
    "    num_demands_missing = df[target_col].isna().sum()\n",
    "    num_demands = num_periods - num_demands_missing\n",
    "    ADI = num_periods / num_demands\n",
    "    df['ADI'] = ADI\n",
    "    \n",
    "    # calculate CV, coefficient of variation = std of population / mean value of population\n",
    "    CV = df[target_col].std() / df.loc[(df[target_col]>0), target_col].mean()\n",
    "    df['CV'] = CV\n",
    "    \n",
    "    # apply rules\n",
    "    df['ts_type'] = 'regular'\n",
    "    if ((ADI < 1.32) & (CV < 0.49)):\n",
    "        df['ts_type'] = 'regular'\n",
    "    elif ((ADI >= 1.32) & (CV < 0.49)):\n",
    "        df['ts_type'] = 'intermittent'     \n",
    "    elif ((ADI < 1.32) & (CV >= 0.49)):\n",
    "        df['ts_type'] = 'erratic'  \n",
    "    elif ((ADI >= 1.32) & (CV >= 0.49)):\n",
    "        df['ts_type'] = 'lumpy'  \n",
    "        \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "#########\n",
    "# Function to plot time series\n",
    "#########\n",
    "\n",
    "def make_plots(theDF, use_location, sample_series):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    \n",
    "    # select range to zoom-in on particular time event\n",
    "    zoomed = theDF.copy()\n",
    "    \n",
    "    # separate plot for each time series entry\n",
    "    num_plots = len(sample_series)\n",
    "       \n",
    "    # resize plots smaller if fewer time series to plot\n",
    "    if num_plots >= 5:    \n",
    "        fig, axs = plt.subplots(num_plots, 1, figsize=(15, 15), sharex=True)\n",
    "    elif num_plots > 1:\n",
    "        fig, axs = plt.subplots(num_plots, 1, figsize=(15, 8), sharex=True)\n",
    "    else:\n",
    "        # default at least 2 plots otherwise axs indexing breaks\n",
    "        fig, axs = plt.subplots(2, 1, figsize=(15, 8), sharex=True)\n",
    "    # fig.subplots_adjust(hspace=0.5) # pad a little if each x-axis has title\n",
    "    \n",
    "    plot_num = 0\n",
    "    for i in range(num_plots):\n",
    "\n",
    "        # get item_id, location_id's target value\n",
    "        if use_location:\n",
    "            item = sample_series.iloc[i, 0]\n",
    "            location = sample_series.iloc[i, 1]\n",
    "            zoomed2 = zoomed.loc[((zoomed[item_id]==item) & (zoomed[location_id]==location)), :].copy()\n",
    "        else:\n",
    "            item = sample_series[i]\n",
    "            zoomed2 = zoomed.loc[((zoomed[item_id]==item)), :].copy()\n",
    "\n",
    "        # plot only if time series exists\n",
    "        if zoomed2.shape[0] > 0:\n",
    "\n",
    "            # plot target_value \n",
    "            zoomed2[[target_value]].plot(ax=axs[plot_num])\n",
    "\n",
    "            # set title for each subplot\n",
    "            if use_location:\n",
    "                axs[plot_num].set_title(f\"item {item} and location {location}\")  \n",
    "            else:\n",
    "                axs[plot_num].set_title(f\"item {item}\")  \n",
    "            fig.text(0.04, 0.5, 'Monthly quantity', va='center', rotation='vertical')\n",
    "\n",
    "            # format the x-axis\n",
    "            axs[plot_num].set_xlabel(\"Timestamp\")  # set x-axis title\n",
    "            axs[plot_num].xaxis.label.set_visible(False)\n",
    "\n",
    "            # remove each individual subplot legend\n",
    "            axs[plot_num].get_legend().remove()\n",
    "\n",
    "            # format the grid\n",
    "            axs[plot_num].grid(False)\n",
    "            axs[plot_num].grid(which='major', axis='x')\n",
    "\n",
    "            # increment plot counter\n",
    "            plot_num = plot_num + 1\n",
    "\n",
    "    plt.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Create a new S3 bucket for this lesson</b>\n",
    "- The cell below will create a new S3 bucket with name ending in \"forecast-demo-taxi\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create unique S3 bucket for saving your own data\n",
    "\n",
    "####\n",
    "# EDIT THIS FOR YOUR DATA\n",
    "####\n",
    "region = boto3.Session(\"s3\").region_name\n",
    "print(f\"region = {region}\")\n",
    "account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "\n",
    "# create unique S3 bucket for saving your own data\n",
    "bucket_name = account_id + '-forecast-demo-taxi'\n",
    "# print(f\"bucket_name = {bucket_name}\")\n",
    "\n",
    "util.create_bucket(bucket_name, region)\n",
    "    \n",
    "# create prefix for organizing your new bucket\n",
    "prefix = \"nyc-taxi-trips\"\n",
    "print(f\"using folder '{prefix}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect API sessions\n",
    "session = boto3.Session(region_name=region) \n",
    "s3 = session.client(service_name='s3')\n",
    "forecast = session.client(service_name='forecast') \n",
    "forecastquery = session.client(service_name='forecastquery')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check you can communicate with Forecast APIs\n",
    "forecast.list_predictors()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Create IAM Role for Forecast</b> <br>\n",
    "Like many AWS services, Forecast will need to assume an IAM role in order to interact with your S3 resources securely. In the sample notebooks, we use the get_or_create_iam_role() utility function to create an IAM role. Please refer to \"notebooks/common/util/fcst_utils.py\" for implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check sagemaker session first, if not found create a role\n",
    "try:\n",
    "    from sagemaker import get_execution_role\n",
    "    role_arn = get_execution_role()\n",
    "except:\n",
    "    # Create the role to provide to Amazon Forecast.\n",
    "    role_name = \"ForecastNotebookRole-Basic\"\n",
    "    print(f\"Creating Role {role_name} ...\")\n",
    "    role_arn = util.get_or_create_iam_role( role_name = role_name )\n",
    "    \n",
    "# echo user inputs without account\n",
    "print(f\"Success! Created role arn = {role_arn.split('/')[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. Read data <a class=\"anchor\" id=\"read\"></a>\n",
    "\n",
    "The first thing we're going to do is read the headerless .csv file.  Then we need to identify which columns map to required Amazon Forecast inputs.\n",
    "\n",
    "<img src=\"https://amazon-forecast-samples.s3-us-west-2.amazonaws.com/common/images/nyctaxi_map_fields.png\" width=\"82%\">\n",
    "<br>\n",
    "\n",
    "<b>In order to use Weather Index, you need a geolocation-type column.</b>  The geolocation-type column connects your locations to geolocations, and can be 5-digit postal code or latitude_longitude.  For more details, see:\n",
    "<ul>\n",
    "    <li><a href=\"https://docs.aws.amazon.com/forecast/latest/dg/weather.html\" target=\"_blank\">Link to documentation about geolocations</a></li>\n",
    "    <li><a href=\"https://aws.amazon.com/blogs/machine-learning/amazon-forecast-weather-index-automatically-include-local-weather-to-increase-your-forecasting-model-accuracy/\" target=\"_blank\">Our Weather blog, which shows UI steps.</a></li>\n",
    "</ul>\n",
    "\n",
    "The cell below shows an example reading headerless csv file with lat_lon geolocation column, \"pickup_geolocation\".  The rest of this notebook writes headerless csv files to be able to use automation.  If you are not planning on using the automation solution, .csv files with headers are allowed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####\n",
    "# EDIT THIS FOR YOUR DATA\n",
    "####\n",
    "\n",
    "## Read cleaned, joined, featurized data from Glue ETL processing\n",
    "df_raw = pd.read_csv(\"s3://amazon-forecast-samples/data_prep_templates/clean_features.csv\"\n",
    "                          , parse_dates=True\n",
    "                          , header=None\n",
    "                          , dtype={0:'str'\n",
    "                                   , 1: 'str'\n",
    "                                   , 2: 'str'\n",
    "                                   , 3:'str'\n",
    "                                   , 4: 'int32'\n",
    "                                   , 5: 'float64'\n",
    "                                   , 6: 'str'\n",
    "                                   , 7: 'str'\n",
    "                                   , 8: 'str'}\n",
    "                          , names=['pulocationid', 'pickup_hourly', 'pickup_day_of_week'\n",
    "                                   , 'day_hour', 'trip_quantity', 'mean_item_loc_weekday'\n",
    "                                   , 'pickup_geolocation', 'pickup_borough', 'binned_max_item'])\n",
    "\n",
    "# drop duplicates\n",
    "print(df_raw.shape)\n",
    "df_raw.drop_duplicates(inplace=True)\n",
    "\n",
    "df_raw['pickup_hourly'] = pd.to_datetime(df_raw[\"pickup_hourly\"], format=\"%Y-%m-%d %H:%M:%S\", errors='coerce')\n",
    "print(df_raw.shape)\n",
    "print(df_raw.dtypes)\n",
    "start_time = df_raw.pickup_hourly.min()\n",
    "end_time = df_raw.pickup_hourly.max()\n",
    "print(f\"Min timestamp = {start_time}\")\n",
    "print(f\"Max timestamp = {end_time}\")\n",
    "df_raw.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####\n",
    "# EDIT THIS FOR YOUR DATA\n",
    "####\n",
    "\n",
    "# map expected column names\n",
    "item_id = \"pulocationid\"\n",
    "target_value = \"trip_quantity\"\n",
    "timestamp = \"pickup_hourly\"\n",
    "location_id = None\n",
    "geolocation = \"pickup_geolocation\"\n",
    "\n",
    "if location_id is None:\n",
    "    use_location = False\n",
    "else:\n",
    "    use_location = True\n",
    "print(f\"use_location = {use_location}\")\n",
    "\n",
    "# specify array of dimensions you'll use for forecasting\n",
    "if use_location:\n",
    "    forecast_dims = [timestamp, location_id, item_id]\n",
    "else:\n",
    "    forecast_dims = [timestamp, item_id]\n",
    "print(f\"forecast_dims = {forecast_dims}\")\n",
    "dims_except_timestamp = [i for i in forecast_dims if i != timestamp]\n",
    "print(f\"dims_except_timestamp = {dims_except_timestamp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_shape = df_raw.shape \n",
    "original_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. Correct dtypes <a class=\"anchor\" id=\"fix_dtypes\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####\n",
    "# EDIT THIS FOR YOUR DATA\n",
    "####\n",
    "# # correct dtypes\n",
    "# df_raw['ReportingMonth'] = pd.to_datetime(df_raw[\"ReportingMonth\"], format=\"%Y-%m-%d\", errors='coerce')\n",
    "# # Use the new pandas Integer type\n",
    "# df_raw.ProductGroup3Id = df_raw.ProductGroup3Id.astype('Int64').astype(str)\n",
    "# # df_raw.ProductGroup3Id = df_raw.ProductGroup3Id.astype('Int64')\n",
    "print(df_raw.shape)\n",
    "print(df_raw.dtypes)\n",
    "\n",
    "# df_raw.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. Make forecast choices <a class=\"anchor\" id=\"choices\"></a>\n",
    "\n",
    "Below, you need to make some choices.  First, you will be asked how you want to treat target values (the values you're forecasting). \n",
    "<ol>\n",
    "    <li><b>Do you want your target values to be floating point numbers or integers?</b>  If you choose floating point numbers, you won't be able to use negative-binomial distribution later in the DeepAR+ algorithm.  If you don't care about that, then just hit enter to accept the default, which is floats. </li>\n",
    "    <li><b>Decide if you want to round 0's to nulls.</b>  If you believe in your data '0' really means '0', then just hit enter to accept the default of do nothing to 0's.  On the other hand, if you think some of your 0's were missing data instead of actual 0's, then opt-in by typing 'yes' to convert 0's to nulls. </li>\n",
    "    <li><b>Decide if you want to automatically impute missing categorical fields?</b> If you think you only have a few missing values, you might impute missing values to be the top category.</li>\n",
    "    <li><b>Decide if you want to replace extreme values with mean?</b>  If you think you only have one or two extreme values it might make sense to replace them. On the other hand, if you have quite a few, that is indication those values are not really extreme.</li>\n",
    "    <li><b>Do you want to generate future RTS that extends into future?</b>  If you set this to True, all data will be used for Training.  If you set it to False, a hold-out of length Forecast Horizon will be used to alculte RTS.</li>\n",
    "    <li><b>How many time units do you want to forecast?</b>. For example, if your time unit is Hour, then if you want to forecast out 1 week, that would be 24*7 = 168 hours, so answer = 168. </li>\n",
    "    <li><b>What is the time granularity for your data?</b>. For example, if your time unit is Hour, answer = \"H\". </li>\n",
    "    <li><b>What is the first date you want to forecast?</b>  Training data will be cut short 1 time unit before the desired first forecast snapshot date.  For example, if the granularity of your data is \"D\" and you want your first forecast to happen on Feb 8, 2019, then the last timestamp for training will be Feb 7, 2019.</li>\n",
    "    <li><b>Think of a name you want to give this experiment</b>, so all files will have the same names.  You should also use this same name for your Forecast DatasetGroup name, to set yourself up for reproducibility. </li>\n",
    "    </ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FORECAST SETTINGS\n",
    "###################\n",
    "# INSTRUCTIONS: \n",
    "# 1. replace occurences \"xxx=+FORECAST_LENGTH\" with correct dictionary value \n",
    "# 2. run this cell 1x, look at suggested \"snapshot_date\"\n",
    "# 3. use suggested \"snapshot_date\", run again.\n",
    "###################\n",
    "\n",
    "# do not round target values to integers\n",
    "target_to_integer = False\n",
    "\n",
    "# replace 0's with nulls\n",
    "replace_all_zeroes_with_null = False\n",
    "\n",
    "# replace extremes with mean of last 3 months\n",
    "replace_extremes_with_mean = False\n",
    "\n",
    "# Create RTS with unknown future data\n",
    "# Note: if you set this to True, all known data will be used for Training\n",
    "# Note: if you set this to False, a hold-out of length Forecast Horizon will be used to calculate RTS\n",
    "create_future_RTS_with_unknown_data = False\n",
    "\n",
    "# What is your forecast time unit granularity?\n",
    "# Choices are: ^Y|M|W|D|H|30min|15min|10min|5min|1min$ \n",
    "FORECAST_FREQ = \"H\"\n",
    "\n",
    "# what is your forecast horizon in number time units you've selected?\n",
    "# e.g. if you're forecasting in hours, how many hours out do you want a forecast?\n",
    "FORECAST_LENGTH = 168\n",
    "\n",
    "# What is the first date you want to forecast? \n",
    "# Training data will be cut short 1 time unit before the desired first forecast snapshot date\n",
    "# get snapshot date (date of 1st forecast) as last time minus forecast horizon\n",
    "AF_freq_to_dateutil_freq = {\"Y\":\"years\", \"M\":\"hours\", \"W\":\"weeks\", \"D\":\"days\", \"H\":\"hours\"}\n",
    "\n",
    "####\n",
    "# EDIT THIS FOR YOUR DATA\n",
    "####\n",
    "###################\n",
    "# INSTRUCTIONS: replace \"xxx=FORECAST_LENGTH\" with correct dictionary value from above\n",
    "#               example if FORECAST_FREQ=\"W\" then use \"weeks=FORECAST_LENGTH\"\n",
    "###################\n",
    "end_time_train = df_raw[timestamp].max() - relativedelta(hours=FORECAST_LENGTH)\n",
    "snapshot_date = end_time_train.date() + relativedelta(days=1)\n",
    "print(f\"Suggested snapshot date = {snapshot_date}\")\n",
    "\n",
    "# Run entire cell 1x, to view the \"Suggested snapshot date\"\n",
    "# Change snapshot date below to match suggested\n",
    "SNAPSHOT_DATE = datetime.datetime(2020, 2, 23, 0, 0, 0)  \n",
    "\n",
    "# What name do you want to give this experiment?  \n",
    "# Be sure to use same name for your Forecast Dataset Group name.\n",
    "EXPERIMENT_NAME = \"nyctaxi_demo2\"\n",
    "DATA_VERSION = 7\n",
    "\n",
    "# print some validation back to user\n",
    "print(f\"Convert your frequency to python dateutil = {AF_freq_to_dateutil_freq[FORECAST_FREQ]}\")\n",
    "print(f\"Forecast horizon = {FORECAST_LENGTH} {AF_freq_to_dateutil_freq[FORECAST_FREQ]}\")\n",
    "\n",
    "AF_freq_to_dateutil_freq = {\"Y\":\"years\", \"M\":\"hours\", \"W\":\"weeks\", \"D\":\"days\", \"H\":\"hours\"}\n",
    "####\n",
    "# EDIT THIS FOR YOUR DATA\n",
    "####\n",
    "###################\n",
    "# INSTRUCTIONS: replace \"xxx=+FORECAST_LENGTH\" with correct dictionary value from above\n",
    "#               example if FORECAST_FREQ=\"W\" then use \"weeks=+FORECAST_LENGTH\"\n",
    "###################\n",
    "snapshot_end = SNAPSHOT_DATE + relativedelta(hours=+FORECAST_LENGTH)\n",
    "snapshot_end = snapshot_end - relativedelta(hours=1)\n",
    "print(f\"Training data end date = {end_time_train}\")\n",
    "print(f\"Forecast start date = {SNAPSHOT_DATE}\")\n",
    "print(f\"Forecast end date = {snapshot_end}\")\n",
    "start_time = df_raw[timestamp].min()\n",
    "end_time = snapshot_end\n",
    "\n",
    "snapshot_date_monthYear = SNAPSHOT_DATE.strftime(\"%m%d%Y\")\n",
    "EXPERIMENT_NAME = f\"{EXPERIMENT_NAME}_snap{snapshot_date_monthYear}_{FORECAST_LENGTH}{FORECAST_FREQ}\"\n",
    "print(f\"Experiment name = {EXPERIMENT_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4. Drop null item_ids <a class=\"anchor\" id=\"drop_null_items\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "templist = df_raw[item_id].unique()\n",
    "print(f\"Number unique items: {len(templist)}\")\n",
    "print(f\"Number nulls: {pd.isnull(templist).sum()}\")\n",
    "\n",
    "if len(templist) < 20:\n",
    "    print(templist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Drop the null item_ids, if any exist\n",
    "if pd.isnull(templist).sum() > 0:\n",
    "    print(df_raw.shape)\n",
    "    df_raw = df_raw.loc[(~df_raw[item_id].isna()), :].copy()\n",
    "    print(df_raw.shape)\n",
    "    print(len(df_raw[item_id].unique()))\n",
    "else:\n",
    "    print(\"No missing item_ids found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5. Drop null timestamps <a class=\"anchor\" id=\"drop_null_times\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check null timestamps\n",
    "templist = df_raw.loc[(df_raw[timestamp].isna()), :].shape[0]\n",
    "print(f\"Number nulls: {templist}\")\n",
    "\n",
    "if (templist < 10) & (templist > 0) :\n",
    "    print(df_raw.loc[(df_raw[timestamp].isna()), :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Drop the null quantities and dates\n",
    "if templist > 0:\n",
    "    print(df_raw.shape)\n",
    "    df_raw = df_raw.loc[(~df_raw[timestamp].isna()), :].copy()\n",
    "    print(df_raw.shape)\n",
    "    print(df_raw['timestamp'].isna().sum())\n",
    "else:\n",
    "    print(\"No null timestamps found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7. Inspect (and treat) extremes <a class=\"anchor\" id=\"treat_extremes\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decide how many extreme values is considered unusual\n",
    "MAX_EXTREMES = 10\n",
    "EXTREME_VALUE = df_raw[target_value].quantile(0.999999)\n",
    "print(EXTREME_VALUE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect extremes\n",
    "templist = df_raw.loc[(df_raw[target_value]>=EXTREME_VALUE), :].shape[0]\n",
    "print(f\"Number extremes: {templist}\")\n",
    "\n",
    "if (templist < MAX_EXTREMES) & (templist > 0) :\n",
    "    print(df_raw.loc[(df_raw[target_value]>=EXTREME_VALUE), :].set_index([item_id]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_extremes_with_mean = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace extreme value with treated median last 6 months value or last 3 months if hourly data\n",
    "MONTHS_TO_USE = 3\n",
    "\n",
    "if replace_extremes_with_mean:\n",
    "\n",
    "    # calculate median value from last 3 months before the extreme\n",
    "    keys = df_raw.loc[(df_raw[target_value]>=EXTREME_VALUE), [timestamp, item_id, location_id]].max()\n",
    "    temp = df_raw.loc[((df_raw[item_id]==keys[item_id]) \n",
    "                       & (df_raw[location_id]==keys[location_id])\n",
    "                       & (df_raw[timestamp]<keys[timestamp])\n",
    "                       & (df_raw[timestamp]>keys[timestamp] - relativedelta(months=MONTHS_TO_USE))), :] \n",
    "    # make sure you've got 6 values if using 6 months\n",
    "#     assert(temp.shape[0]==MONTHS_TO_USE)\n",
    "    # calculate median\n",
    "    replace_extreme = temp[target_value].median() #np.round(temp[target_value].mean(),0)\n",
    "    print(replace_extreme)\n",
    "    # visually check if replaced value looks like median target_value\n",
    "    display(temp)\n",
    "else:\n",
    "    print(\"Do not replace any extreme values.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PLOT OVERALL TIME SERIES - TO HELP INSPECT EXTREMES\n",
    "df_raw.plot(x=timestamp, y=target_value, figsize=(15, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace extreme value with treated median last 6 months value or last 3 months if hourly data\n",
    "\n",
    "if (replace_extremes_with_mean & (templist < MAX_EXTREMES)):\n",
    "    df_clean = df_raw.copy()\n",
    "    # calculate median\n",
    "    replace_extreme = temp[target_value].median()\n",
    "    print(f\"replacing extreme value {df_clean.loc[(df_clean[target_value]>=EXTREME_VALUE), target_value].max()} with {replace_extreme}\")\n",
    "    # make the replacement\n",
    "    df_clean.loc[((df_clean[item_id]==keys[item_id]) \n",
    "                       & (df_clean[location_id]==keys[location_id])\n",
    "                       & (df_clean[timestamp]==keys[timestamp])), target_value] = replace_extreme\n",
    "    print(f\"new value is {df_clean.loc[((df_clean[item_id]==keys[item_id]) & (df_clean[location_id]==keys[location_id]) & (df_clean[timestamp]==keys[timestamp])), target_value].max()}\")\n",
    "    \n",
    "    # Inspect extremes again on clean data\n",
    "    EXTREME_VALUE = df_raw[target_value].quantile(0.999999)\n",
    "    print(EXTREME_VALUE)\n",
    "\n",
    "    templist = df_clean.loc[(df_clean[target_value]>=EXTREME_VALUE), :].shape[0]\n",
    "    print(f\"Number extremes: {templist}\")\n",
    "\n",
    "    if (templist < MAX_EXTREMES) & (templist > 0) :\n",
    "        print(df_clean.loc[(df_clean[target_value]>=EXTREME_VALUE), :].set_index([item_id]))\n",
    "        \n",
    "    ## PLOT OVERALL TIME SERIES - TO SEE IF YOU DID THE RIGHT THING\n",
    "    df_clean.plot(x=timestamp, y=target_value, figsize=(15, 8))\n",
    "else:\n",
    "    print(\"No extreme values found or do not replace any extremes.\")\n",
    "    df_clean = df_raw.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save some memory\n",
    "del df_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# throws error if we lost some values\n",
    "assert original_shape[0] == df_clean.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8. Optional - Round negative targets up to 0 <a class=\"anchor\" id=\"round_negatives\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check negative values\n",
    "print(df_clean.loc[(df_clean[target_value] <0), :].shape)\n",
    "df_clean.loc[(df_clean[target_value] <0), :].sort_values([timestamp, item_id]).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CAREFUL!!  MAKE SURE ROUNDING NEGATIVES UP TO 0 MAKES SENSE FOR YOUR USE CASE\n",
    "\n",
    "# If negative values found, round them up to 0\n",
    "if df_clean.loc[(df_clean[target_value] <0), :].shape[0] > 0:\n",
    "\n",
    "    # Check y-value before cleaning\n",
    "    print(df_clean[target_value].describe())\n",
    "\n",
    "    # default negative values in demand to 0\n",
    "    print(f\"{df_clean[target_value].lt(0).sum()} negative values will be rounded up to 0\")\n",
    "    print()\n",
    "    ts_cols = [target_value]\n",
    "\n",
    "    for c in ts_cols:\n",
    "        df_clean.loc[(df_clean[c] < 0.0), c] = 0.0\n",
    "\n",
    "    # Check y-value after cleaning\n",
    "    print(df_clean[target_value].describe())\n",
    "else:\n",
    "    print(\"No negative values found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9. Optional - Convert negative targets to nan <a class=\"anchor\" id=\"negatives_to_nan\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check negative values\n",
    "# print(df_clean.loc[(df_clean[target_value] <0), :].shape)\n",
    "# df_clean.loc[(df_clean[target_value] <0), :].sort_values([timestamp, item_id]).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # CAREFUL!!  MAKE SURE CHANGING NEGATIVES TO NAN NEGATIVES MAKES SENSE FOR YOUR USE CASE\n",
    "\n",
    "# # If negative values found, round them up to 0\n",
    "# if df_clean.loc[(df_clean[target_value] <0), :].shape[0] > 0:\n",
    "\n",
    "#     # Check y-value before cleaning\n",
    "#     print(df_clean[target_value].describe())\n",
    "\n",
    "#     # default negative values in demand to 0\n",
    "#     print(f\"{df_clean[target_value].lt(0).sum()} negative values will be rounded up to 0\")\n",
    "#     print()\n",
    "#     ts_cols = [target_value]\n",
    "    \n",
    "#     print ()\n",
    "\n",
    "#     for c in ts_cols:\n",
    "#         df_clean.loc[(df_clean[c] < 0.0), c] = float('nan')\n",
    "        \n",
    "        \n",
    "\n",
    "#     # Check y-value after cleaning\n",
    "#     print(df_clean[target_value].describe())\n",
    "# else:\n",
    "#     print(\"No negative values found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # throws error if we lost some values\n",
    "# assert original_shape[0] == df_clean.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10. Aggregate at your chosen frequency <a class=\"anchor\" id=\"groupby_frequency\"></a>\n",
    "\n",
    "Below, we show an example of resampling at hourly frequency by forecast dimensions.  Modify the code to resample at other frequencies.\n",
    "\n",
    "Decide which aggregation-level makes sense for your data, which is a balance between desired aggregation and what the data-collection frequency will support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## CHECK TO SEE IF YOUR TIMESERIES DIMENSIONS ARE CORRECT\n",
    "\n",
    "# checking if there are multiple entries per item_id per timestamp per location\n",
    "df_aux = df_clean.copy().set_index(forecast_dims)\n",
    "\n",
    "duplicates = df_aux.pivot_table(index=forecast_dims, aggfunc='size')\n",
    "duplicates = pd.DataFrame( duplicates, columns=[\"NumberPerTS\"])\n",
    "\n",
    "print (duplicates[duplicates[\"NumberPerTS\"]>1].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking to see if your timeseries dimensions are correct\n",
    "if duplicates[duplicates[\"NumberPerTS\"]>1].shape[0] > 0:\n",
    "    print(\"WARNING:  YOUR AGGREGATION ASSUMPTION THAT timestamp, item_id, location_id ARE UNIQUE IS NOT CORRECT.\")\n",
    "    print(\"Inspect df_aux where you see 'NumberPerTS' > 1\")\n",
    "else:\n",
    "    print(\"Success!  timestamp, item_id, location_id is a unique grouping of your time series.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case your assumed dimensions are not unique, code below is to explore adding a composite column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####\n",
    "# EDIT THIS FOR YOUR DATA\n",
    "####\n",
    "\n",
    "# #  inspect what is happening on these repeated items\n",
    "# try:\n",
    "#     df_aux.reset_index(inplace = True)\n",
    "# except Exception as e:\n",
    "#     print (e)\n",
    "\n",
    "# test_aux = df_aux.loc[((df_aux[item_id]=='PRD-05685') & (df_aux[location_id]=='STCK-00605')\n",
    "#                       & (df_aux[timestamp]==\"2019-09-30 01:36:45\")), :]\n",
    "# test_aux.sort_values(by=[timestamp])\n",
    "\n",
    "# # Possibly extra dimension Organization Name?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Since location is not unique, create new fake composite column\n",
    "# df_clean['timeseries_key'] = df_clean[item_id] + '-'+ df_clean[location_id] \\\n",
    "#                                 + '-' + df_clean['Organization Name']\n",
    "# df_clean.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # If you changed dimensions, re-map expected column names\n",
    "# item_id = \"timeseries_key\"\n",
    "# use_location = False\n",
    "# forecast_dims = [timestamp, item_id]\n",
    "# print(f\"forecast_dims = {forecast_dims}\")\n",
    "# dims_except_timestamp = [i for i in forecast_dims if i != timestamp]\n",
    "# print(f\"dims_except_timestamp = {dims_except_timestamp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## CHECK AGAIN TO SEE IF YOUR DATA AGGREGATION ASSUMPTION IS CORRECT\n",
    "\n",
    "# # checking if there are multiple entries per item_id per timestamp per location\n",
    "# df_aux = df_clean[forecast_dims + [target_value]].copy().set_index(forecast_dims)\n",
    "# df_aux.drop_duplicates(inplace=True)\n",
    "\n",
    "# duplicates = df_aux.pivot_table(index=forecast_dims, aggfunc='size')\n",
    "# duplicates = pd.DataFrame( duplicates, columns=[\"NumberPerTS\"])\n",
    "\n",
    "# # checking to see if your data aggregation is correct\n",
    "# if duplicates[duplicates[\"NumberPerTS\"]>1].shape[0] > 0:\n",
    "#     print (duplicates[duplicates[\"NumberPerTS\"]>1].head())\n",
    "#     print(\"WARNING:  YOUR AGGREGATION ASSUMPTION THAT timestamp, item_id, location_id ARE UNIQUE IS NOT CORRECT.\")\n",
    "#     print(\"Inspect df_aux where you see 'NumberPerTS' > 1\")\n",
    "# else:\n",
    "#     print(\"Success!  timestamp, item_id, location_id is a unique grouping of your time series.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####\n",
    "# EDIT THIS FOR YOUR DATA\n",
    "####\n",
    "\n",
    "# restrict columns if desired\n",
    "# df_clean = df_clean[[timestamp, item_id, \"Prod #\", location_id, \"Organization Name\", target_value]].copy()\n",
    "print(df_clean.shape)\n",
    "df_clean.drop_duplicates(inplace=True)\n",
    "print(df_clean.shape)\n",
    "\n",
    "# put all cols besides forecast_dims in a dictionary of agg fns you want to keep in the df\n",
    "agg_dict = {\"pickup_day_of_week\":\"first\", \"day_hour\":\"first\"\n",
    "            ,\"trip_quantity\":\"sum\", \"mean_item_loc_weekday\":\"mean\", \"pickup_geolocation\":\"first\"\n",
    "            , \"pickup_borough\":\"first\", \"binned_max_item\":\"last\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # THIS CODE BLOCK IS AN EXAMPLE OF Weekly AGGREGATION\n",
    "# # def aggregate_time_series(theDF, agg_freq, timestamp_col, item_col, target_col, agg_dict, already_grouped):\n",
    "# #   already_grouped = True if theDF timestamp is already at the desired aggregation level\n",
    "# #                                ; False otherwise\n",
    "# g_week = aggregate_time_series(df_clean, \"W\", timestamp, item_id, target_value, agg_dict, False)\n",
    "\n",
    "# # add new time dimension since original timestamp is not weekly\n",
    "# g_week['year_week'] = g_week[timestamp].dt.year.astype(str) + '_' \\\n",
    "#                         + g_week[timestamp].dt.isocalendar().week.astype(str)\n",
    "# display(g_week.sample(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS CODE BLOCK IS AN EXAMPLE OF Hourly AGGREGATION\n",
    "# Note:  The sample data shipped with notebook is ideal - all time series have 5856 data points, \n",
    "# which is squarely in the Deep Learning desired data size.\n",
    "\n",
    "# def aggregate_time_series(theDF, agg_freq, timestamp_col, item_col, target_col, agg_dict, already_grouped):\n",
    "#   already_grouped = True if theDF timestamp is already at the desired aggregation level\n",
    "#                                ; False otherwise\n",
    "g_hour = aggregate_time_series(df_clean, \"H\", timestamp, item_id, target_value, agg_dict, True)\n",
    "display(g_hour.sample(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TRY ANOTHER AGGREGATION LEVEL AND COMPARE TARGET_VALUE DISTRIBUTION SHAPES\n",
    "agg_freq = \"2H\"\n",
    "\n",
    "g_2hour = aggregate_time_series(df_clean, \"2H\", timestamp, item_id, target_value, agg_dict, False)\n",
    "display(g_2hour.sample(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TRY ANOTHER AGGREGATION LEVEL AND COMPARE TARGET_VALUE DISTRIBUTION SHAPES\n",
    "agg_freq = \"4H\"\n",
    "\n",
    "g_4hour = aggregate_time_series(df_clean, \"4H\", timestamp, item_id, target_value, agg_dict, False)\n",
    "display(g_4hour.sample(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<b> Select the aggregation-level to keep, based on results above.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## USE THE GROUPING YOU SELECTED ABOVE\n",
    "\n",
    "df = g_hour.copy()\n",
    "\n",
    "# save memory\n",
    "del g_hour\n",
    "\n",
    "print(df.shape, df_clean.shape)\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## USE THE GROUPING YOU SELECTED ABOVE\n",
    "\n",
    "# df = g_week.copy()\n",
    "\n",
    "# # save memory\n",
    "# del g_week\n",
    "\n",
    "# print(df.shape, df_clean.shape)\n",
    "# df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11. Typical retail scenarios: Find top-moving items <a class=\"anchor\" id=\"top_moving_items\"></a>\n",
    "\n",
    "Next, we want to drill down and visualize some individual item time series.  Typically customers have \"catalog-type\" data, where only the top 20% of their items are top-movers; the rest of the 80% of items are not top-movers.  For visualization, we want to select automatically some of the top-moving items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## FIND TOP-MOVING ITEMS\n",
    "\n",
    "if use_location:\n",
    "    print(\"using location_id\")\n",
    "    df_velocity, df_ts_velocity = get_top_moving_items(df, timestamp, target_value, item_id, location_id)\n",
    "else:\n",
    "    print(\"using only item_id\")\n",
    "    df_velocity, df_ts_velocity = get_top_moving_items(df, timestamp, target_value, item_id, None)\n",
    "\n",
    "# Display breakdown: how many top-moving items\n",
    "if use_location:\n",
    "    num_top = df_ts_velocity.loc[(df_ts_velocity.top_moving==True), :].groupby(dims_except_timestamp).first().shape[0]\n",
    "    num_slow = df_ts_velocity.loc[(df_ts_velocity.top_moving==False), :].groupby(dims_except_timestamp).first().shape[0]\n",
    "    num_time_series = df.groupby(dims_except_timestamp).first().shape[0]\n",
    "    print(f\"number of top moving time series: {num_top}, ratio:{np.round(num_top/num_time_series,2)}\")\n",
    "    print(f\"number of slow moving time series: {num_slow}, ratio: {np.round(num_slow/num_time_series,2)}\")\n",
    "    print()\n",
    "    top_moving_ts = df_ts_velocity.loc[(df_ts_velocity.top_moving==True), :]\n",
    "    slow_moving_ts = df_ts_velocity.loc[(df_ts_velocity.top_moving==False), :]\n",
    "    print(\"Top-moving time series\")\n",
    "    print(top_moving_ts.sort_values('velocity', ascending=False).head(3))\n",
    "    print()\n",
    "    print(\"Slow-moving time series\")\n",
    "    print(slow_moving_ts.sort_values('velocity', ascending=False).head(2))\n",
    "else:\n",
    "    num_top = df_velocity.loc[(df_velocity.top_moving==True), item_id].nunique()\n",
    "    num_slow = df_velocity.loc[(df_velocity.top_moving==False), item_id].nunique()\n",
    "    num_time_series = df[item_id].nunique()\n",
    "    print(f\"number of top moving items: {num_top}, ratio:{num_top/num_time_series}\")\n",
    "    print(f\"number of slow moving items: {num_slow}, ratio: {num_slow/num_time_series}\")\n",
    "    print(\"Top-moving items\")\n",
    "    top_moving_items = df_velocity.loc[(df_velocity.top_moving==True), :].sort_values('velocity', ascending=False).copy()\n",
    "    print(top_moving_items.head(3))\n",
    "    print()\n",
    "    print(\"Slow-moving items\")\n",
    "    slow_moving_items = df_velocity.loc[(df_velocity.top_moving==False), :].sort_values('velocity', ascending=False).copy()\n",
    "    print(slow_moving_items.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instead of completely random items, select top-moving items\n",
    "if use_location:\n",
    "    random_series = top_moving_ts.groupby([item_id, location_id])[item_id, location_id].first()\n",
    "    random_series.reset_index(inplace=True, drop=True)\n",
    "    random_series = random_series.sample(5)\n",
    "    print(random_series)\n",
    "else:\n",
    "    random_items = top_moving_items[item_id].sample(5)\n",
    "    random_items = random_items.reset_index()\n",
    "    random_items = random_items[item_id]\n",
    "    print(random_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Remove this for your data...\n",
    "\n",
    "# random_items = list(random_items) + ['79', '135']\n",
    "# random_items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step12. Visualize time series <a class=\"anchor\" id=\"visualize\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREPARE PLOTS OF RANDOM TOP-MOVING ITEMS AND LOCATIONS\n",
    "# We chose top-moving items to avoid visualizing time series that might be empty or very sparse.\n",
    "\n",
    "if use_location:\n",
    "    df_plot = df.loc[(df[item_id].isin(random_series[item_id].unique())\n",
    "                     & (df[location_id].isin(random_series[location_id].unique())))\n",
    "                     , [item_id, timestamp, target_value, location_id]].copy()\n",
    "else:\n",
    "    df_plot = df.loc[(df[item_id].isin(random_items)), [item_id, timestamp, target_value]].copy()  \n",
    "\n",
    "df_plot.set_index(timestamp, inplace=True)\n",
    "df_plot.head(2)\n",
    "\n",
    "\n",
    "# make plots\n",
    "if use_location:\n",
    "    make_plots(df_plot, use_location, random_series)\n",
    "else:\n",
    "    make_plots(df_plot, use_location, random_items)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 14. Split train/test data  <a class=\"anchor\" id=\"split_train_test\"></a> \n",
    "\n",
    "In forecasting, \"train\" data is until a last-train date, sometimes called the forecast snapshot date.  \n",
    "<ul>\n",
    "    <li>Train data includes all data up to your last-train date. </li>\n",
    "    <li>Test data includes dates after your last-train date through end of desired forecast horizon.</li>\n",
    "    <li>Validation data might exist for part or maybe all of the desired forecast horizon. </li>\n",
    "    <li>TTS timestamps should start and end with Train data. </li>\n",
    "    <li>RTS timestamps should start with Train data and extend out past end of TTS to end of the desired forecast horizon.</li>\n",
    "    </ul>\n",
    "    \n",
    "For model generalization, all processing from here on out will only be done on train data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forecast Horizon is number of time steps out in the future you want to predict\n",
    "# Time steps are defined in the time frequency you specified in Step 5 Aggregate\n",
    "\n",
    "# Example if aggregation was hourly, then forecast length=168 means forecast horizon of 7 days or 7*24=168 hours\n",
    "print(f\"Forecast horizon = {FORECAST_LENGTH}\") # = 12\n",
    "print(f\"Forecast unit of frequency = {AF_freq_to_dateutil_freq[FORECAST_FREQ]}\") # = 30\n",
    "print(f\"Forecast start date = {SNAPSHOT_DATE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train data as all except last FORECAST_HORIZON length\n",
    "start_time = df[timestamp].min()\n",
    "end_time = snapshot_end\n",
    "start_time_test = SNAPSHOT_DATE\n",
    "\n",
    "print(f\"start_time = {start_time}\")\n",
    "print(f\"end_time_train = {end_time_train}\")\n",
    "print(f\"start_time_test = {start_time_test}\")\n",
    "print(f\"end_time = {snapshot_end}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_future_RTS_with_unknown_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if create_future_RTS_with_unknown_data:\n",
    "    # Create train data as all data => this means RTS will extend into unknown future\n",
    "    print(\"using all known data for training\")\n",
    "    train_df = df.copy()\n",
    "else:\n",
    "    # Create train subset with hold-out of length FORECAST_LENGTH\n",
    "    print(\"using hold-out with train data\")\n",
    "    train_df = df.copy()\n",
    "    train_df = train_df.loc[(train_df[timestamp] <= end_time_train), :]\n",
    "\n",
    "# check you did the right thing\n",
    "print(f\"start_time = {start_time}\")\n",
    "print(f\"end_time: {end_time}\")\n",
    "print()\n",
    "print(f\"start_time_train = {train_df[timestamp].min()}\")\n",
    "print(f\"end_time_train = {train_df[timestamp].max()}\")\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ERROR CHECK: DO YOU HAVE ENOUGH HISTORICAL DATA POINTS TO SUPPORT DESIRED FORECAST HORIZON?\n",
    "\n",
    "# calculate number data points in train data\n",
    "num_data_points = train_df.groupby(dims_except_timestamp).nunique()[timestamp].mean()\n",
    "print(f\"1/3 training data points: {np.round(num_data_points/3,0)}\")\n",
    "\n",
    "# Amazon Forecast length of forecasts can be 500 data points and 1/3 target time series dataset length\n",
    "if ((FORECAST_LENGTH < 500) & (FORECAST_LENGTH <= np.round(num_data_points/3,0))):\n",
    "    print(f\"Success, forecast horizon {FORECAST_LENGTH} is shorter than 500 data points and 1/3 train data\")\n",
    "else:\n",
    "    print(f\"Error, forecast horizon {FORECAST_LENGTH} is too long.  Need fewer than 500 data points and 1/3 train data. \")\n",
    "    \n",
    "# If you have too few data points, return to step above and choose smaller time granularity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 15. Prepare and Save Target Time Series (TTS) <a class=\"anchor\" id=\"TTS\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(create_future_RTS_with_unknown_data)\n",
    "train_df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> TODO:  Customize the dictionary below to map your columns and desired aggregations </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Assemble TTS required columns\n",
    "\n",
    "####\n",
    "# EDIT THIS FOR YOUR DATA\n",
    "####\n",
    "if geolocation is not None:\n",
    "    print(\"running g_else\")\n",
    "    # restrict train data to just tts columns\n",
    "    tts = train_df[[timestamp, item_id, geolocation, target_value]].copy()\n",
    "    tts = tts.groupby(forecast_dims)[[target_value, geolocation]].sum()\n",
    "else:\n",
    "    print(\"running else\")\n",
    "    # restrict train data to just tts columns\n",
    "    tts = train_df[[timestamp, item_id, target_value]].copy()\n",
    "    tts = tts.groupby(forecast_dims)[[target_value]].sum()\n",
    "    \n",
    "tts.reset_index(inplace=True)\n",
    "print(f\"start date = {tts[timestamp].min()}\")\n",
    "print(f\"end date = {tts[timestamp].max()}\")\n",
    "\n",
    "# check it\n",
    "print(tts.shape)\n",
    "print(tts.dtypes)\n",
    "tts.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check format of geolocation column\n",
    "# tts[geolocation].value_counts(normalize=True, dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Optional - convert target_value to integer if this is the last step for TTS. </b>\n",
    "\n",
    "Note: Currently in Amazon Forecast, if you declare target_value is integer in the schema, but you have any decimals in your numbers, you will get an error.\n",
    "\n",
    "Make sure you really see integers in the code below, if you want integers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_to_integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the new pandas Integer type\n",
    "# https://pandas.pydata.org/pandas-docs/stable/user_guide/integer_na.html\n",
    "# TODO: turn this into a function\n",
    "\n",
    "if target_to_integer:\n",
    "    try:\n",
    "        tts[target_value] = tts[target_value].astype(int)\n",
    "        print(\"Success! Converted to numpy integer\")\n",
    "    except Exception as e:\n",
    "        print (e)\n",
    "        print(\"Trying pandas nullable Integer type instead of numpy integer type...\")\n",
    "        try:\n",
    "            tts[target_value] = tts[target_value].astype('Int64', errors='ignore')\n",
    "            print(\"Success! converted to pandas integer\")\n",
    "        except Exception as e:\n",
    "            print (e)\n",
    "elif tts[target_value].dtype == 'object':\n",
    "    # convert to float\n",
    "    tts[target_value] = tts[target_value].astype(np.float32)\n",
    "elif tts[target_value].dtype != 'object':\n",
    "    # do nothing\n",
    "    print(\"target_value is already a float\")\n",
    "    \n",
    "print(tts.dtypes)\n",
    "tts.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save tts to S3\n",
    "# local_file = \"tts.csv\"\n",
    "# # Save merged file locally\n",
    "# tts.to_csv(local_file, header=False, index=False)\n",
    "\n",
    "# key = f\"{prefix}/v{DATA_VERSION}/{EXPERIMENT_NAME}.csv\"\n",
    "# boto3.Session().resource('s3').Bucket(bucket_name).Object(key).upload_file(local_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 16. Remove time series with no target values at all<a class=\"anchor\" id=\"#TTS_remove_all0\"></a>\n",
    "In case there are time series which are only 0's, may as well remove them, since their forecast should be all 0's too.  Another reason to remove these time series is they could bias the overall forecast toward 0, when that's not what you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tts.shape)\n",
    "tts.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if sum of all sales is 0\n",
    "g = tts.groupby(dims_except_timestamp).sum()\n",
    "g.fillna(0, inplace=True)\n",
    "skus_with_no_sales_in_warehouse = g[g[target_value] == 0].copy()\n",
    "\n",
    "# drop extra columns for cleaner merge\n",
    "skus_with_no_sales_in_warehouse.reset_index(inplace=True)\n",
    "skus_with_no_sales_in_warehouse = skus_with_no_sales_in_warehouse.iloc[:, 0:1]\n",
    "skus_with_no_sales_in_warehouse.drop_duplicates(inplace=True)\n",
    "display (skus_with_no_sales_in_warehouse.head(2))\n",
    "\n",
    "if skus_with_no_sales_in_warehouse.shape[0] > 0:\n",
    "        \n",
    "    # https://stackoverflow.com/questions/32676027/how-to-do-df1-not-df2-dataframe-merge-in-pandas\n",
    "    tts_copy = tts.merge(skus_with_no_sales_in_warehouse, how='left', on=dims_except_timestamp, indicator=True) \\\n",
    "               .query(\"_merge=='left_only'\") \\\n",
    "               .drop('_merge',1)\n",
    "\n",
    "    print(tts.shape, tts_copy.shape)\n",
    "    display(tts_copy.sample(5))\n",
    "\n",
    "else:\n",
    "    print(\"No time series found with only 0's.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# really drop skus with only 0's\n",
    "\n",
    "if skus_with_no_sales_in_warehouse.shape[0] > 0:\n",
    "    print(tts.shape, tts_copy.shape)\n",
    "    tts = tts_copy.copy()\n",
    "    del (tts_copy)\n",
    "    \n",
    "    # keep track of dropped dimensions and reason why dropped\n",
    "    skus_with_no_sales_in_warehouse = skus_with_no_sales_in_warehouse[dims_except_timestamp].copy()\n",
    "    \n",
    "    # save the reason\n",
    "    skus_with_no_sales_in_warehouse['reason'] = \"All 0's\"\n",
    "    display(skus_with_no_sales_in_warehouse.head(2))\n",
    "    \n",
    "    print(tts.shape)\n",
    "    display(tts.sample(5))\n",
    "\n",
    "else:\n",
    "    print(\"Didn't drop anything\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 17. Remove time series with end of life<a class=\"anchor\" id=\"TTS-remove-all_end_of_life\"></a>\n",
    "\n",
    "Check if time series have any data in last 6 months and more than 5 data points, since 5 data points is minimum for Amazon Forecast to generate forecasts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tts.shape)\n",
    "tts.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define end of life = No sales in the last 6 months\n",
    "\n",
    "# first get df of only last 6 months\n",
    "time_threshold = end_time - datetime.timedelta(6*30) \n",
    "\n",
    "# check if sum of sales last 6 months is 0\n",
    "tts_aux = tts[tts[timestamp] >= time_threshold].copy()\n",
    "g = tts_aux.groupby(dims_except_timestamp).sum()\n",
    "g.fillna(0, inplace=True)\n",
    "skus_with_end_of_life = g[g[target_value] == 0].copy()\n",
    "\n",
    "# drop extra columns for cleaner merge\n",
    "skus_with_end_of_life.reset_index(inplace=True)\n",
    "skus_with_end_of_life = skus_with_end_of_life.iloc[:, 0:1]\n",
    "skus_with_end_of_life.drop_duplicates(inplace=True)\n",
    "display (skus_with_end_of_life.head(2))\n",
    "\n",
    "if skus_with_end_of_life.shape[0] > 0:\n",
    "\n",
    "    # https://stackoverflow.com/questions/32676027/how-to-do-df1-not-df2-dataframe-merge-in-pandas\n",
    "    tts_copy = tts.merge(skus_with_end_of_life, how='left', on=dims_except_timestamp, indicator=True) \\\n",
    "               .query(\"_merge=='left_only'\") \\\n",
    "               .drop('_merge',1)\n",
    "\n",
    "    print(tts.shape, tts_copy.shape)\n",
    "    display(tts_copy.sample(5))\n",
    "\n",
    "else:\n",
    "    print(\"No time series found with end of life.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# really drop the skus with end of life\n",
    "\n",
    "if skus_with_end_of_life.shape[0] > 0:\n",
    "    print(tts.shape, tts_copy.shape)\n",
    "    tts = tts_copy.copy()\n",
    "    del (tts_copy)\n",
    "    display(tts.dtypes)\n",
    "    \n",
    "    # keep track of dropped dimensions and reason\n",
    "    skus_with_end_of_life = skus_with_end_of_life[dims_except_timestamp].copy()\n",
    "    skus_with_end_of_life['reason'] = \"end of life\"\n",
    "    display(skus_with_end_of_life.head(2))\n",
    "    \n",
    "    print(tts.shape)\n",
    "    display(tts.sample(5))\n",
    "    \n",
    "else:\n",
    "    print(\"Didn't drop anything\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 18. Remove time series with fewer than 5 data points<a class=\"anchor\" id=\"TTS_remove_too_few_data_points\"></a>\n",
    "\n",
    "Note:  Minimum number of data points is 5 data points to make a forecast.  You can run this to remove them manually and save the list of time series with too few data points for your own reference.  Otherwise if you skip this section, Forecast will automatically drop (silently) all time series with fewer than 5 data points, since that is too few to make a good forecast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tts.shape)\n",
    "tts.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Replacing '0's with null</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_all_zeroes_with_null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Null-value filling, if any\n",
    "\n",
    "# special case:  replace 0s with nulls\n",
    "if (replace_all_zeroes_with_null):\n",
    "    print(tts.shape)\n",
    "    print(tts[target_value].describe())\n",
    "    if target_to_integer:\n",
    "        tts.loc[(tts[target_value]==0), target_value] = pd.NA\n",
    "    else:\n",
    "        tts.loc[(tts[target_value]==0), target_value] = np.nan\n",
    "    print ()\n",
    "    print(tts.shape)\n",
    "    print(tts[target_value].describe())\n",
    "else:\n",
    "    tts.loc[:, target_value].fillna(0, inplace=True)\n",
    "    print(\"No null-filling required.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check per time series if count of data points is at least 5\n",
    "g = tts.groupby(dims_except_timestamp).count()\n",
    "skus_with_too_few_sales = g[g[target_value] < 5].copy()\n",
    "\n",
    "# drop extra columns for cleaner merge\n",
    "skus_with_too_few_sales.reset_index(inplace=True)\n",
    "skus_with_too_few_sales = skus_with_too_few_sales.iloc[:, 0:1]\n",
    "skus_with_too_few_sales.drop_duplicates(inplace=True)\n",
    "display (skus_with_too_few_sales.head(2))\n",
    "\n",
    "if skus_with_too_few_sales.shape[0] > 0:\n",
    "\n",
    "    # https://stackoverflow.com/questions/32676027/how-to-do-df1-not-df2-dataframe-merge-in-pandas\n",
    "    tts_copy = tts.merge(skus_with_too_few_sales, how='left', on=dims_except_timestamp, indicator=True) \\\n",
    "               .query(\"_merge=='left_only'\") \\\n",
    "               .drop('_merge',1)\n",
    "\n",
    "    print(\"TTS if you dropped items with too few data points\")\n",
    "    print(tts.shape, tts_copy.shape)\n",
    "    display(tts_copy.sample(5))\n",
    "\n",
    "else:\n",
    "    print(\"No time series found with fewer than 5 datapoints.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# really drop skus with too few data points, only if more than a handful found\n",
    "\n",
    "if skus_with_too_few_sales.shape[0] > 0:\n",
    "    print(tts.shape, tts_copy.shape)\n",
    "    tts = tts_copy.copy()\n",
    "    del (tts_copy)\n",
    "    \n",
    "    # keep track of dropped dimensions and reason why dropped\n",
    "    skus_with_too_few_sales = skus_with_too_few_sales[dims_except_timestamp].copy()\n",
    "    skus_with_too_few_sales['reason'] = \"Fewer than 5 datapoints\"\n",
    "    display(skus_with_too_few_sales.head(2))\n",
    "    \n",
    "    print(tts.shape)\n",
    "    display(tts.sample(5))\n",
    "    \n",
    "else:\n",
    "    print(\"Didn't drop anything\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Keep track of dropped time series and reason why they were dropped. </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if skus_with_too_few_sales.shape[0] > 0:\n",
    "    dropped_dims = skus_with_too_few_sales.append([skus_with_no_sales_in_warehouse\n",
    "                                                       , skus_with_end_of_life])\n",
    "    print(f\"unique ts dropped = {dropped_dims.shape[0]}\")\n",
    "    print(f\"unique ts fewer than 5 data points = {skus_with_too_few_sales.shape[0]}\")\n",
    "    print(f\"unique ts with all 0s = {skus_with_no_sales_in_warehouse.shape[0]}\")\n",
    "    print(f\"unique ts with end of life = {skus_with_end_of_life.shape[0]}\")\n",
    "    display(dropped_dims.reason.value_counts(dropna=False, normalize=True))\n",
    "    display(dropped_dims.sample(1))\n",
    "else:\n",
    "    print(\"Didn't drop anything\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save list of dropped skus and reasons for reference and to check if data can be fixed\n",
    "\n",
    "if skus_with_too_few_sales.shape[0] > 2:\n",
    "    # save all the dropped dimensions fields\n",
    "    local_file = \"dropped_fields.csv\"\n",
    "    # Save merged file locally\n",
    "    dropped_dims.to_csv(local_file, header=True, index=False)\n",
    "\n",
    "    key = f\"{prefix}/v{DATA_VERSION}/dropped_{EXPERIMENT_NAME}.csv\"\n",
    "    boto3.Session().resource('s3').Bucket(bucket_name).Object(key).upload_file(local_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Optional - convert target_value to integer if this is last step for TTS. </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tts.shape)\n",
    "display(tts.dtypes)\n",
    "tts.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_to_integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the new pandas Integer type\n",
    "# https://pandas.pydata.org/pandas-docs/stable/user_guide/integer_na.html\n",
    "\n",
    "if target_to_integer:\n",
    "    try:\n",
    "        tts[target_value] = tts[target_value].fillna(0).astype(int)\n",
    "        print(\"Success! Converted to np.integer type\")\n",
    "    except Exception as e:\n",
    "        print (e)\n",
    "        print(\"Trying pandas nullable Integer type instead of numpy integer type...\")\n",
    "        try:\n",
    "            tts[target_value] = tts[target_value].astype('Int64', errors='ignore')\n",
    "            print(\"Success! Converted to nullable pd.integer type\")\n",
    "        except Exception as e:\n",
    "            print (e)\n",
    "elif tts[target_value].dtype == 'object':\n",
    "    # convert to float\n",
    "    tts[target_value] = tts[target_value].astype(np.float32)\n",
    "elif tts[target_value].dtype != 'object':\n",
    "    # do nothing\n",
    "    print(\"target_value is already a float\")\n",
    "    \n",
    "print(tts.dtypes)\n",
    "tts.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Optional - replace 0's with nulls </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_all_zeroes_with_null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if replace_all_zeroes_with_null:\n",
    "    tts.loc[(tts[target_value]==0), target_value] = pd.NA\n",
    "    \n",
    "print(tts[target_value].describe())\n",
    "print(tts.dtypes)\n",
    "tts.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one last check...\n",
    "tts = tts[forecast_dims + [target_value]].copy()\n",
    "print(tts.shape)\n",
    "tts.drop_duplicates(inplace=True)\n",
    "print(tts.shape)\n",
    "print(tts[timestamp].min())\n",
    "print(tts[timestamp].max())\n",
    "# check for nulls\n",
    "print(tts.isnull().sum())\n",
    "print(tts.dtypes)\n",
    "tts.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save tts to S3\n",
    "local_file = \"tts.csv\"\n",
    "# Save merged file locally\n",
    "tts.to_csv(local_file, header=False, index=False)\n",
    "\n",
    "key = f\"{prefix}/v{DATA_VERSION}/{EXPERIMENT_NAME}.csv\"\n",
    "boto3.Session().resource('s3').Bucket(bucket_name).Object(key).upload_file(local_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check input numbers of time series\n",
    "if skus_with_too_few_sales.shape[0] > 0:\n",
    "    dropped = dropped_dims.groupby(dims_except_timestamp).first().shape[0]\n",
    "    display(dropped)\n",
    "    # check\n",
    "    assert (train_df.groupby(dims_except_timestamp).first().shape[0] \\\n",
    "            == (tts.groupby(dims_except_timestamp).first().shape[0] + dropped))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 19. Optional - Assemble and save TTS_sparse, TTS_dense <a class=\"anchor\" id=\"TTS-dense_sparse\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_dims\n",
    "dims_except_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num data points per item\n",
    "sparsity = tts.copy()\n",
    "sparsity = sparsity.groupby(dims_except_timestamp).count()\n",
    "\n",
    "sparsity[[target_value]].hist(bins=45);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = sparsity[target_value].describe()\n",
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list the \"dense\" item combinations\n",
    "sparsity.reset_index(inplace=True)\n",
    "dense = sparsity.loc[(sparsity[target_value] >= stats['75%']), forecast_dims].drop_duplicates()\n",
    "print(dense.shape, sparsity.drop_duplicates().shape)\n",
    "dense.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all the rest are sparse\n",
    "sparse = sparsity.merge(dense, how='outer', indicator=True) \\\n",
    "           .query(\"_merge=='left_only'\") \\\n",
    "           .drop('_merge',1)[forecast_dims]\n",
    "sparse = sparsity.loc[(sparsity[target_value] < stats['75%']), forecast_dims].drop_duplicates()\n",
    "\n",
    "print(sparse.shape, dense.shape, sparsity.drop_duplicates().shape)\n",
    "sparse.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# peek at some singletons\n",
    "# sparsity.loc[(sparsity[target_value] <5), [item_id, location_id]].drop_duplicates().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # spot-check a singleton\n",
    "# df.loc[((df[item_id]==\"204\")), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save sparse dimensions\n",
    "local_file = \"sparse_fields.csv\"\n",
    "# Save merged file locally\n",
    "sparse.to_csv(local_file, header=False, index=False)\n",
    "\n",
    "key = f\"{prefix}/v{DATA_VERSION}/sparse_{EXPERIMENT_NAME}.csv\"\n",
    "boto3.Session().resource('s3').Bucket(bucket_name).Object(key).upload_file(local_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save dense dimensions\n",
    "local_file = \"dense_fields.csv\"\n",
    "# Save merged file locally\n",
    "dense.to_csv(local_file, header=False, index=False)\n",
    "\n",
    "key = f\"{prefix}/v{DATA_VERSION}/dense_{EXPERIMENT_NAME}.csv\"\n",
    "boto3.Session().resource('s3').Bucket(bucket_name).Object(key).upload_file(local_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restrict tts to just dense time series\n",
    "if use_location:\n",
    "    tts_dense = tts.loc[(tts[item_id].isin(dense[item_id].unique())\n",
    "                        & (tts[location_id].isin(dense[location_id].unique()))),:].copy()\n",
    "else:\n",
    "    tts_dense = tts.loc[(tts[item_id].isin(dense[item_id].unique())),:].copy()\n",
    "print(tts_dense.shape, tts.shape)\n",
    "tts_dense.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 20. Optional - Assemble and save tts.top, tts.slow <a class=\"anchor\" id=\"TTS_top\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assemble tts_top\n",
    "if use_location:\n",
    "    tts_top = tts.loc[(tts[item_id].isin(top_moving_ts[item_id].unique())\n",
    "                        & (tts[location_id].isin(top_moving_ts[location_id].unique()))),:].copy()\n",
    "else:\n",
    "    tts_top = tts.loc[(tts[item_id].isin(top_moving_items[item_id].unique())),:].copy()\n",
    "\n",
    "print(tts_top.shape, tts.shape)\n",
    "num_top_items = tts_top.groupby(dims_except_timestamp).first().shape[0]\n",
    "print(f\"Number top items = {num_top_items}\")\n",
    "tts_top.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save tts_top to S3\n",
    "local_file = \"tts_top.csv\"\n",
    "# Save merged file locally\n",
    "tts_top.to_csv(local_file, header=False, index=False)\n",
    "\n",
    "key = f\"{prefix}/v{DATA_VERSION}/tts_top_{EXPERIMENT_NAME}.csv\"\n",
    "boto3.Session().resource('s3').Bucket(bucket_name).Object(key).upload_file(local_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assemble tts_slow\n",
    "if use_location:\n",
    "    tts_slow = tts.loc[(tts[item_id].isin(slow_moving_ts[item_id].unique())\n",
    "                        & (tts[location_id].isin(slow_moving_ts[location_id].unique()))),:].copy()\n",
    "else:\n",
    "    tts_slow = tts.loc[(tts[item_id].isin(slow_moving_items[item_id].unique())),:].copy()\n",
    "\n",
    "\n",
    "print(tts_slow.shape, tts.shape)\n",
    "num_slow_items = tts_slow.groupby(dims_except_timestamp).first().shape[0]\n",
    "print(f\"Number slow items = {num_slow_items}\")\n",
    "tts_slow.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save tts_slow to S3\n",
    "local_file = \"tts_slow.csv\"\n",
    "# Save merged file locally\n",
    "tts_slow.to_csv(local_file, header=False, index=False)\n",
    "\n",
    "key = f\"{prefix}/v{DATA_VERSION}/tts_slow_{EXPERIMENT_NAME}.csv\"\n",
    "boto3.Session().resource('s3').Bucket(bucket_name).Object(key).upload_file(local_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 23. Prepare and save RTS (if any) <a class=\"anchor\" id=\"RTS\"></a>\n",
    "\n",
    "Make sure RTS does not have any missing values, even if RTS extends into future. <br>\n",
    "Trick:  create dataframe without any missing values using cross-join, faster than resample technique. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you get memory allocation error in merges below, try overriding default value 0 to 1 for overcommit\n",
    "# see https://www.kernel.org/doc/Documentation/vm/overcommit-accounting\n",
    "# Might have to do this directly in terminal\n",
    "# !sudo -i \n",
    "# !echo 1 > /proc/sys/vm/overcommit_memory\n",
    "!cat /proc/sys/vm/overcommit_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = pd.date_range(start=start_time, end=end_time, freq=FORECAST_FREQ)\n",
    "all_times = pd.DataFrame(index=idx)\n",
    "print (f\"Number of data points: {len(all_times.index)}\")\n",
    "print (f\"Start date = {all_times.index.min()}\")\n",
    "print (f\"End date = {all_times.index.max()}\")\n",
    "\n",
    "# Create timestamp column\n",
    "all_times.reset_index(inplace=True)\n",
    "all_times.columns = [timestamp]\n",
    "\n",
    "# # Create other time-related columns if you need them in RTS\n",
    "# all_times['year_week'] = all_times[timestamp].dt.year.astype(str) + '_' + all_times[timestamp].dt.month.astype(str)\n",
    "\n",
    "print(all_times.dtypes)\n",
    "print(all_times.isna().sum())\n",
    "print(all_times.shape)\n",
    "all_times.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create master template of all possible locations and items\n",
    "items = tts.groupby([item_id])[[item_id]].min()\n",
    "# print(len(items))\n",
    "\n",
    "if use_location:\n",
    "    if geolocation is not None:\n",
    "        print(f\"found geolocation {geolocation}\")\n",
    "        locations = pd.DataFrame(list(tts[location_id, geolocation].unique()))\n",
    "        locations.columns = [location_id, geolocation]\n",
    "    else:\n",
    "        locations = pd.DataFrame(list(tts[location_id].unique()))\n",
    "        locations.columns = [location_id]\n",
    "\n",
    "    #     print(len(locations))\n",
    "    locations['key'] = 1\n",
    "    items['key'] = 1\n",
    "    # Do the cross-join\n",
    "    master_records = locations.merge(items, on ='key').drop(\"key\", 1) \n",
    "    print(master_records.shape, items.shape, locations.shape)\n",
    "    num_locs = master_records[location_id].nunique()\n",
    "    print(f\"num locations = {num_locs}\")\n",
    "else:\n",
    "    master_records = items.copy()\n",
    "    print(master_records.shape, items.shape)\n",
    "\n",
    "# check you did the right thing\n",
    "num_items = master_records[item_id].nunique()\n",
    "print(f\"num items = {num_items}\")\n",
    "master_records.tail()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# cross-join to create master template of all possible locations and items and times\n",
    "all_times['key'] = \"1\"\n",
    "master_records['key'] = \"1\"\n",
    "all_times.set_index('key', inplace=True)\n",
    "master_records.set_index('key', inplace=True)\n",
    "\n",
    "# Do the cross-join\n",
    "print(\"doing the merge...\")\n",
    "full_history = master_records.merge(all_times, how=\"outer\", left_index=True, right_index=True)\n",
    "print(\"done w/ merge...\")\n",
    "full_history.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# make sure you don't have any nulls\n",
    "print(full_history.shape)\n",
    "print(\"checking nulls...\")\n",
    "print(full_history.isna().sum())\n",
    "full_history.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add in original target_value\n",
    "\n",
    "# small df of target_values\n",
    "temp_target = df[forecast_dims + [target_value]].copy()\n",
    "# add key for faster join\n",
    "if use_location:\n",
    "    temp_target['ts_key'] = temp_target[timestamp].astype(str) + \"-\" + temp_target[item_id] + \"-\" + temp_target[location_id]\n",
    "else:\n",
    "    temp_target['ts_key'] = temp_target[timestamp].astype(str) + \"-\" + temp_target[item_id]\n",
    "temp_target = temp_target.groupby('ts_key').sum()\n",
    "# temp_target.drop(forecast_dims, inplace=True, axis=1)\n",
    "# temp_target.set_index('ts_key', inplace=True)\n",
    "print(temp_target.shape, df.shape)\n",
    "display(temp_target.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete all local files to free up disk space\n",
    "!rm *.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Parallelization for faster merge </b><br>\n",
    "Below, I used dask.  I also tried ray through modin library.  I found error when adding a new column to modin dataframe.  Maybe by the time you use this notebook the modin/ray problem will be solved.\n",
    "https://github.com/modin-project/modin/issues/2442\n",
    "\n",
    "For reference, here are dask best practices:\n",
    "<ul>\n",
    "    <li>Choose partitions to be #items if your time series have more dimensions than just item_id, see <a href=\"https://docs.dask.org/en/latest/best-practices.html\" target=\"_blank\">https://docs.dask.org/en/latest/best-practices.html</a></li>\n",
    "    <li>Make sure reset_index is only done in pandas and not dask, see <a href=\"https://docs.dask.org/en/latest/dataframe-best-practices.html\" target=\"_blank\">https://docs.dask.org/en/latest/dataframe-best-practices.html</a></li>\n",
    "    </ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dask for faster joins\n",
    "!pip install \"dask[dataframe]\" \n",
    "import dask\n",
    "import dask.dataframe as dd\n",
    "from dask.diagnostics import ProgressBar\n",
    "print('dask: {}'.format(dask.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# add in original target_value\n",
    "\n",
    "# USING dask\n",
    "\n",
    "# large df full_history\n",
    "temp2 = full_history.copy()\n",
    "\n",
    "# convert large pandas df to dask df\n",
    "print(type(temp2))\n",
    "if use_location:\n",
    "    num_partitions = num_items\n",
    "else:\n",
    "    num_partitions = 1\n",
    "print(f\"using num_partitions = {num_partitions}\")\n",
    "large_df = dd.from_pandas(temp2, npartitions=num_partitions)\n",
    "print(type(large_df))\n",
    "\n",
    "# add key for faster join\n",
    "if use_location:\n",
    "    large_df['ts_key'] = large_df[timestamp].astype(str) + \"-\" + large_df[item_id] + \"-\" + large_df[location_id]\n",
    "else:\n",
    "    large_df['ts_key'] = large_df[timestamp].astype(str) + \"-\" + large_df[item_id]   \n",
    "print(large_df.shape, full_history.shape)\n",
    "display(large_df.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "# add in original target_value\n",
    "\n",
    "# temp = pd.concat([temp2.set_index('ts_key'), temp_target.set_index('ts_key')], axis=1)\n",
    "# temp = pd.concat([temp2, temp_target], keys=\"ts_key\", axis=1)\n",
    "# drop level, drop null index\n",
    "# temp.columns = temp.columns.droplevel()\n",
    "# temp = temp.iloc[:, 1:].copy()\n",
    "temp = large_df.merge(temp_target, how=\"left\", right_index=True, left_on=\"ts_key\")\n",
    "print(temp.shape, full_history.shape)\n",
    "display(temp.head(3))\n",
    "\n",
    "# CPU times: user 637 ms, sys: 8.18 ms, total: 645 ms\n",
    "# Wall time: 640 ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "# convert dask df back to pandas df\n",
    "# # Below is too small !?  rows got dropped, why?\n",
    "print(type(temp))\n",
    "temp3 = temp.compute()\n",
    "print(type(temp3))\n",
    "temp3.drop('ts_key', axis=1, inplace=True)\n",
    "print(temp3.shape, full_history.shape)\n",
    "\n",
    "# check nulls\n",
    "print(temp3.isna().sum())\n",
    "display(temp3.sample(3))\n",
    "temp3[target_value].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Careful!!\n",
    "# Really replace full_history with merged values\n",
    "print(full_history.shape)\n",
    "full_history = temp3.copy()\n",
    "print(full_history.shape)\n",
    "print(type(full_history))\n",
    "del temp, temp_target, temp2, temp3\n",
    "full_history.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####\n",
    "# EDIT THIS FOR YOUR DATA\n",
    "####\n",
    "\n",
    "# # Create other time-related columns if you need them in RTS\n",
    "# full_history['month'] = full_history[timestamp].dt.month.astype(str)\n",
    "# full_history['year'] = full_history[timestamp].dt.year.astype(str)\n",
    "# full_history['quarter'] = full_history[timestamp].dt.quarter.astype(str)\n",
    "# full_history['year_month'] = full_history['year'] + '_' + full_history['month']\n",
    "# full_history['year_quarter'] = full_history['year'] + '_' + full_history['quarter']\n",
    "\n",
    "# Here are some candidates for hourly data\n",
    "full_history['day_of_week'] = full_history[timestamp].dt.day_name().astype(str)\n",
    "full_history['hour_of_day'] = full_history[timestamp].dt.hour.astype(str)\n",
    "full_history['day_hour_name'] = full_history['day_of_week'] + \"_\" + full_history['hour_of_day']\n",
    "full_history['weekend_flag'] = full_history[timestamp].dt.dayofweek\n",
    "full_history['weekend_flag'] = (full_history['weekend_flag'] >= 5).astype(int)\n",
    "full_history['is_sun_mon'] = 0\n",
    "full_history.loc[((full_history.day_of_week==\"Sunday\") | (full_history.day_of_week==\"Monday\")), 'is_sun_mon'] = 1\n",
    "\n",
    "print(full_history.sample(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example - create feature from target_value that is sometimes useful\n",
    "\n",
    "# # calculate mean sales per item per year\n",
    "# TODO: add normalization here\n",
    "\n",
    "# temp_year_item = train_df[['year', item_id, target_value]].copy()\n",
    "# temp_year_item.year = temp_year_item.year.astype(str)\n",
    "# temp_year_item = temp_year_item.groupby(['year', item_id]).mean()\n",
    "# temp_year_item.reset_index(inplace=True)\n",
    "# temp_year_item.rename(columns={target_value:\"count_year_item\"}, inplace=True)\n",
    "# print(temp_year_item.dtypes)\n",
    "# temp_year_item.sample(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # merge in year-item trend\n",
    "\n",
    "# temp2 = full_history.copy()\n",
    "# # temp.drop(\"count_day_loc_item\", inplace=True, axis=1)\n",
    "# print(temp2.shape)\n",
    "# temp = temp2.merge(temp_year_item, how=\"left\", on=[\"year\", item_id])\n",
    "# print(temp.shape, temp_year_item.shape)\n",
    "\n",
    "# # check nulls\n",
    "# print(temp.isna().sum())\n",
    "# temp.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Careful!!\n",
    "# # Really replace full_history with merged values\n",
    "# full_history = temp.copy()\n",
    "# full_history.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zoom-in time slice so you can see patterns\n",
    "df_plot = full_history.loc[(full_history[item_id].isin(random_items)), :].copy()\n",
    "df_plot = full_history.loc[((full_history[timestamp]>\"2020-01-10\")\n",
    "                           & (full_history[timestamp]<end_time_train)\n",
    "                           & (full_history[item_id].isin(random_items))), :].copy()\n",
    "print(df_plot.shape, full_history.shape)\n",
    "df_plot = df_plot.groupby([timestamp]).sum()\n",
    "df_plot.reset_index(inplace=True)\n",
    "df_plot.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check: target_value distribution in full dataframe looks same as original\n",
    "df_plot[target_value].hist(bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXAMPLE HOURLY RTS\n",
    "\n",
    "# Visualize candidate RTS variables\n",
    "plt.figure(figsize=(15, 8))\n",
    "ax = plt.gca()\n",
    "df_plot.plot(x=timestamp, y=target_value, ax=ax);\n",
    "ax2 = ax.twinx()\n",
    "df_plot.plot(x=timestamp, y='weekend_flag', color='red', alpha=0.3, ax=ax2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXAMPLE HOURLY RTS\n",
    "\n",
    "# Visualize candidate RTS variables is_sun_mon\n",
    "plt.figure(figsize=(15, 8))\n",
    "ax = plt.gca()\n",
    "df_plot.plot(x=timestamp, y=target_value, ax=ax);\n",
    "ax2 = ax.twinx()\n",
    "df_plot.plot(x=timestamp, y='is_sun_mon', color='red', alpha=0.3, ax=ax2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXAMPLE HOURLY RTS\n",
    "\n",
    "# Zoom-in.  Visualize candidate RTS variables is_sun_mon\n",
    "plt.figure(figsize=(15, 8))\n",
    "ax = plt.gca()\n",
    "df_plot.loc[(df_plot[timestamp]>\"2020-02-06\"), :].plot(x=timestamp, y=target_value, ax=ax);\n",
    "ax2 = ax.twinx()\n",
    "df_plot.loc[(df_plot[timestamp]>\"2020-02-06\"), :].plot(x=timestamp, y='is_sun_mon', color='red', alpha=0.3, ax=ax2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like lowest taxis rides are wee hours of Sunday morning through Mondays.  So it's a combination of day and hour that seems to matter, not just day of week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXAMPLE HOURLY RTS\n",
    "\n",
    "# Assemble RTS - include whatever columns you finally decide\n",
    "rts = full_history[forecast_dims + ['day_hour_name']].copy()\n",
    "\n",
    "print(rts.shape)\n",
    "print(rts.isnull().sum())\n",
    "print(f\"rts start: {rts[timestamp].min()}\")\n",
    "print(f\"rts end: {rts[timestamp].max()}\")\n",
    "rts.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save rts to S3\n",
    "local_file = \"rts.csv\"\n",
    "# Save merged file locally\n",
    "rts.to_csv(local_file, header=False, index=False)\n",
    "\n",
    "key = f\"{prefix}/v{DATA_VERSION}/{EXPERIMENT_NAME}.related.csv\"\n",
    "boto3.Session().resource('s3').Bucket(bucket_name).Object(key).upload_file(local_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 13. Classify Time Series <a class=\"anchor\" id=\"Classify\"></a>\n",
    "Using definitions given here:  https://frepple.com/blog/demand-classification/ and here: https://support.demandcaster.com/hc/en-us/articles/360043259931-Forecast-Structure<br>\n",
    "Original article:  https://robjhyndman.com/papers/idcat <br>\n",
    "\n",
    "Idea:  Based on demand patterns, time series can be classified into one of 4 classes:  Regular, Intermittent, Erratic, or Lumpy.  If you have more than 1 class of time series in your data, this might suggest more than 1 model for your time series predictions.  \n",
    "\n",
    "Rules:\n",
    "<ol>\n",
    "    <li><b>Regular</b> demand (ADI < 1.32 and CV² < 0.49). Regular in time and in quantity. It is therefore easy to forecast and you won’t have trouble reaching a low forecasting error level. Suggested algorithm: <b>Traditional statistical such as Exponential Smoothing, Prophet, or ARIMA.</b></li>\n",
    "    <li><b>Intermittent demand</b> (ADI >= 1.32 and CV² < 0.49). The demand history shows very little variation in demand quantity but a high variation in the interval between two demands. Though specific forecasting methods tackle intermittent demands, the forecast error margin is considerably higher. Suggested algorithm: <b>Croston smoothing or some newer research approach coupled with adjusted error metric over longer time period.</b></li>\n",
    "    <li><b>Erratic</b> demand (ADI < 1.32 and CV² >= 0.49). The demand has regular occurrences in time with high quantity variations. Your forecast accuracy remains shaky. Suggested algorithm: <b>Deep Learning</b></li>\n",
    "<li><b>Lumpy</b> demand (ADI >= 1.32 and CV² >= 0.49). The demand is characterized by a large variation in quantity and in time. It is actually impossible to produce a reliable forecast, no matter which forecasting tools you use. This particular type of demand pattern is unforecastable. Suggested algorithm: <b>bootstrap</b></li>\n",
    "    </ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install squarify\n",
    "import squarify\n",
    "\n",
    "# SCRATCH - choose 4 colors\n",
    "print()\n",
    "print(\"THIS IS JUST A TEST TO CHECK COLOR CHOICES...\")\n",
    "colors = colorblind6[0:4]\n",
    "sizes = [40, 30, 5, 25]\n",
    "squarify.plot(sizes, color=colors)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # For each time series, calculate ADI using pandas apply()\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "\n",
    "# temp = full_history.copy()\n",
    "# temp = temp.groupby(dims_except_timestamp).apply(lambda x: calc_ADI(x, timestamp, item_id, target_value))\n",
    "\n",
    "# temp.reset_index(drop=True, inplace=True)\n",
    "# display(temp.sample(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For big data, above is taking too long.  Try dask parallelism instead.<br>\n",
    "<b>TODO: Customize meta dictionary below according to columns in full_history</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_history.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# convert large pandas df to dask df\n",
    "print(type(full_history))\n",
    "if use_location:\n",
    "    num_partitions = num_items\n",
    "else:\n",
    "    num_partitions = 1\n",
    "print(f\"using num_partitions = {num_partitions}\")\n",
    "large_df = dd.from_pandas(full_history, npartitions=num_partitions)\n",
    "print(type(large_df))\n",
    "\n",
    "####\n",
    "# EDIT THIS FOR YOUR DATA\n",
    "####\n",
    "# try parallelizing using dask\n",
    "temp = large_df\\\n",
    "    .groupby([item_id])\\\n",
    "    .apply(lambda x: calc_ADI(x, timestamp, item_id, target_value),\n",
    "           meta={item_id: \"object\",\n",
    "                 timestamp: \"object\",\n",
    "                 \"trip_quantity\": \"f4\",\n",
    "                 \"day_of_week\": \"object\",\n",
    "                 \"hour_of_day\": \"object\", \n",
    "                 \"day_hour_name\": \"object\", \n",
    "                 \"weekend_flag\": \"object\", \n",
    "                 \"is_sun_mon\": \"object\", \n",
    "                 \"ADI\": \"f4\", \n",
    "                 \"CV\": \"f4\", \n",
    "                 \"ts_type\": \"f4\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# convert dask df back to pandas df\n",
    "temp2 = temp.compute()\n",
    "# del temp\n",
    "print(type(temp2))\n",
    "print(temp2.shape, full_history.shape)\n",
    "temp2.reset_index(drop=True, inplace=True)\n",
    "display(temp2.sample(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp2.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capture each time series class counts\n",
    "ts_counts = temp2.groupby(dims_except_timestamp)[['ts_type']].first()\n",
    "ts_counts.reset_index(inplace=True)\n",
    "ts_counts = ts_counts.ts_type.value_counts(dropna=False)\n",
    "print(\"TIME SERIES CLASSES: \")\n",
    "display(ts_counts)\n",
    "print(f\"NUMBER OF DISTINCT CLASSES OF TIME SERIES = {temp2.ts_type.nunique()}\")\n",
    "print(temp2.shape, full_history.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Careful!!\n",
    "# Really replace full_history with class values\n",
    "full_history = temp2.copy()\n",
    "del temp2\n",
    "full_history.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the time series classes using tree map\n",
    "ts_classes = pd.DataFrame(ts_counts)\n",
    "ts_classes.reset_index(inplace=True, drop=False)\n",
    "ts_classes.columns = ['ts_type', 'ts_type_count']\n",
    "sum_total = ts_classes['ts_type_count'].sum()\n",
    "ts_classes['ts_type_percent'] = 100.0 * ts_classes['ts_type_count'] / sum_total\n",
    "ts_classes['ts_type_percent'] = ts_classes['ts_type_percent'].round(2)\n",
    "display(ts_classes)\n",
    "\n",
    "# tree map\n",
    "lbl = [f\"{c[0]} = \\n{c[1]}%\" for c in zip(ts_classes.ts_type, ts_classes.ts_type_percent)]\n",
    "squarify.plot(\n",
    "    label=lbl\n",
    "    , sizes=list(ts_classes.ts_type_count)\n",
    "    , color=colors\n",
    "    , text_kwargs={'fontsize':14});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print(\"SHOWING ERRATIC TIME SERIES - SAMPLE(5)\")\n",
    "    # visualize some erratic time series\n",
    "    erratic_sample = full_history.loc[(full_history.ts_type==\"erratic\"), forecast_dims + [target_value]]\n",
    "    erratic_items = list(erratic_sample[item_id].unique())\n",
    "    erratic_sample = tts.loc[(tts[item_id].isin(erratic_items)), :].copy()\n",
    "    erratic_sample.set_index(timestamp, inplace=True)\n",
    "    erratic_sample.fillna(0, inplace=True)\n",
    "    display(erratic_sample.head(2))\n",
    "    # random sample 5\n",
    "    erratic_items = erratic_sample[item_id].sample(5)\n",
    "    # make plots\n",
    "    make_plots(erratic_sample, False, erratic_items.sample(5))\n",
    "except:\n",
    "    print(\"No erratic time series found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print(\"SHOWING REGULAR TIME SERIES - SAMPLE(5)\")\n",
    "    # visualize some regular time series\n",
    "    regular_sample = full_history.loc[(full_history.ts_type==\"regular\"), forecast_dims + [target_value]]\n",
    "    regular_items = list(regular_sample[item_id].unique())\n",
    "    regular_sample = tts.loc[(tts[item_id].isin(regular_items)), :].copy()\n",
    "    regular_sample.set_index(timestamp, inplace=True)\n",
    "    regular_sample.fillna(0, inplace=True)\n",
    "    display(regular_sample.head(2))\n",
    "    # random sample 5\n",
    "    regular_items = regular_sample[item_id].sample(5)\n",
    "    # make plots\n",
    "    make_plots(regular_sample, False, regular_items.sample(5))\n",
    "except:\n",
    "    print(\"No regular time series found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print(\"SHOWING INTERMITTENT TIME SERIES - SAMPLE(5)\")\n",
    "    # visualize some intermittent time series\n",
    "    intermittent_sample = full_history.loc[(full_history.ts_type==\"intermittent\"), forecast_dims + [target_value]]\n",
    "    intermittent_items = list(intermittent_sample[item_id].unique())\n",
    "    intermittent_sample = tts.loc[(tts[item_id].isin(intermittent_items)), :].copy()\n",
    "    intermittent_sample.set_index(timestamp, inplace=True)\n",
    "    intermittent_sample.fillna(0, inplace=True)\n",
    "    display(intermittent_sample.head(2))\n",
    "    # random sample 5\n",
    "    intermittent_items = intermittent_sample[item_id].sample(5)\n",
    "    # make plots\n",
    "    make_plots(intermittent_sample, False, intermittent_items.sample(5))\n",
    "except:\n",
    "    print(\"No intermittent time series found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print(\"SHOWING LUMPY TIME SERIES - SAMPLE(5)\")\n",
    "    # visualize some lumpy time series\n",
    "    lumpy_sample = full_history.loc[(full_history.ts_type==\"lumpy\"), forecast_dims + [target_value]]\n",
    "    lumpy_items = list(lumpy_sample[item_id].unique())\n",
    "#     lumpy_sample = tts.loc[(tts[item_id].isin(lumpy_items)), :].copy()\n",
    "    lumpy_sample.set_index(timestamp, inplace=True)\n",
    "    lumpy_sample.fillna(0, inplace=True)\n",
    "    display(lumpy_sample.head(2))\n",
    "    # random sample 5\n",
    "    lumpy_items = lumpy_sample[item_id].sample(5)\n",
    "    # make plots\n",
    "    make_plots(lumpy_sample, False, lumpy_items.sample(5))\n",
    "except:\n",
    "    print(\"No lumpy time series found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like only \"erratic\" time series are worth predicting.  All the rest look either suspiciously fake or too few data points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 21. Optional - Assemble and save TTS_regular, TTS_erratic, TTS_intermittent, TTS_lumpy <a class=\"anchor\" id=\"TTS_classes\"></a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restrict tts to just regular time series\n",
    "if use_location:\n",
    "    tts_regular = tts.loc[(tts[item_id].isin(list(regular_sample[item_id, location_id].unique()))), :].copy()\n",
    "else:\n",
    "    tts_regular = tts.loc[(tts[item_id].isin(list(regular_sample[item_id].unique()))), :].copy()\n",
    "\n",
    "print(tts_regular.shape, tts.shape)\n",
    "num_regular_items = tts_regular[item_id].nunique()\n",
    "print(f\"Number regular items = {num_regular_items}\")\n",
    "\n",
    "if num_regular_items > 0:\n",
    "    display(tts_regular.sample(5))\n",
    "else:\n",
    "    print(\"No regular time series found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restrict tts to just erratic time series\n",
    "if use_location:\n",
    "    tts_erratic = tts.loc[(tts[item_id].isin(list(erratic_sample[item_id, location_id].unique()))), :].copy()\n",
    "else:\n",
    "    tts_erratic = tts.loc[(tts[item_id].isin(list(erratic_sample[item_id].unique()))), :].copy()\n",
    "\n",
    "print(tts_erratic.shape, tts.shape)\n",
    "num_erratic_items = tts_erratic[item_id].nunique()\n",
    "print(f\"Number erratic items = {num_erratic_items}\")\n",
    "\n",
    "if num_erratic_items > 0:\n",
    "    display(tts_erratic.sample(5))\n",
    "else:\n",
    "    print(\"No erratic time series found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restrict tts to just intermittent time series\n",
    "if use_location:\n",
    "    tts_intermittent = tts.loc[(tts[item_id].isin(list(intermittent_sample[item_id, location_id].unique()))), :].copy()\n",
    "else:\n",
    "    tts_intermittent = tts.loc[(tts[item_id].isin(list(intermittent_sample[item_id].unique()))), :].copy()\n",
    "\n",
    "print(tts_intermittent.shape, tts.shape)\n",
    "num_intermittent_items = tts_intermittent[item_id].nunique()\n",
    "print(f\"Number intermittent items = {num_intermittent_items}\")\n",
    "\n",
    "if num_intermittent_items > 0:\n",
    "    display(tts_intermittent.sample(5))\n",
    "else:\n",
    "    print(\"No intermittent time series found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restrict tts to just lumpy time series\n",
    "if use_location:\n",
    "    tts_lumpy = tts.loc[(tts[item_id].isin(list(lumpy_sample[item_id, location_id].unique()))), :].copy()\n",
    "else:\n",
    "    tts_lumpy = tts.loc[(tts[item_id].isin(list(lumpy_sample[item_id].unique()))), :].copy()\n",
    "\n",
    "print(tts_lumpy.shape, tts.shape)\n",
    "num_lumpy_items = tts_lumpy[item_id].nunique()\n",
    "print(f\"Number lumpy items = {num_lumpy_items}\")\n",
    "\n",
    "if num_lumpy_items > 0:\n",
    "    display(tts_lumpy.sample(5))\n",
    "else:\n",
    "    print(\"No lumpy time series found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save tts_regular to S3\n",
    "local_file = \"tts_regular.csv\"\n",
    "# Save merged file locally\n",
    "tts_regular.to_csv(local_file, header=False, index=False)\n",
    "\n",
    "key = f\"{prefix}/v{DATA_VERSION}/tts_regular_{EXPERIMENT_NAME}.csv\"\n",
    "boto3.Session().resource('s3').Bucket(bucket_name).Object(key).upload_file(local_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save tts_erratic to S3\n",
    "local_file = \"tts_erratic.csv\"\n",
    "# Save merged file locally\n",
    "tts_erratic.to_csv(local_file, header=False, index=False)\n",
    "\n",
    "key = f\"{prefix}/v{DATA_VERSION}/tts_erratic_{EXPERIMENT_NAME}.csv\"\n",
    "boto3.Session().resource('s3').Bucket(bucket_name).Object(key).upload_file(local_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save tts_intermittent to S3\n",
    "local_file = \"tts_intermittent.csv\"\n",
    "# Save merged file locally\n",
    "tts_intermittent.to_csv(local_file, header=False, index=False)\n",
    "\n",
    "key = f\"{prefix}/v{DATA_VERSION}/tts_intermittent_{EXPERIMENT_NAME}.csv\"\n",
    "boto3.Session().resource('s3').Bucket(bucket_name).Object(key).upload_file(local_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save tts_lumpy to S3\n",
    "local_file = \"tts_lumpy.csv\"\n",
    "# Save merged file locally\n",
    "tts_lumpy.to_csv(local_file, header=False, index=False)\n",
    "\n",
    "key = f\"{prefix}/v{DATA_VERSION}/tts_lumpy_{EXPERIMENT_NAME}.csv\"\n",
    "boto3.Session().resource('s3').Bucket(bucket_name).Object(key).upload_file(local_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check\n",
    "assert num_regular_items + num_erratic_items + num_intermittent_items + num_lumpy_items == tts[item_id].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 22. Assemble and save metadata (if any) <a class=\"anchor\" id=\"IM\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify metadata columns\n",
    "# im = df[[item_id, 'pickup_borough', 'binned_max_item']].copy()\n",
    "im = df[[item_id]].copy()\n",
    "im = im.groupby(dims_except_timestamp).first()\n",
    "im.reset_index(inplace=True)\n",
    "# check nulls\n",
    "display(im.isnull().sum())\n",
    "im.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional metadata created by binning just item target_value is sometimes useful.\n",
    "\n",
    "# aggregate sales by item \n",
    "synthetic = df.copy()\n",
    "synthetic = (synthetic.groupby(item_id)\n",
    "        .agg({target_value: ['max']}))\n",
    "\n",
    "synthetic = synthetic.reset_index()\n",
    "synthetic.sample(5)\n",
    "\n",
    "#bin data into 4 categories\n",
    "cat_scales = [\"Cat_{}\".format(i) for i in range(1,5)]\n",
    "synthetic['item_cat_by_max'] = list(pd.cut(synthetic[target_value]['max'].values, 4, labels=cat_scales))\n",
    "\n",
    "synthetic.drop(target_value, axis=1, inplace=True)\n",
    "synthetic.columns = synthetic.columns.get_level_values(0)\n",
    "\n",
    "print(synthetic.shape)\n",
    "print(synthetic.dtypes)\n",
    "print(synthetic.columns)\n",
    "display(synthetic.sample(5))\n",
    "print(synthetic.item_cat_by_max.value_counts(dropna=False))\n",
    "\n",
    "# merge synthetic features\n",
    "im = im.merge(synthetic, how=\"left\", on=[item_id])\n",
    "print(im.shape, synthetic.shape)\n",
    "im.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check metadata so far\n",
    "\n",
    "print(im.shape)\n",
    "if im.shape[0] < 50:\n",
    "    display(im)\n",
    "else:\n",
    "    display(im.head())\n",
    "\n",
    "# check cardinality of metadata columns\n",
    "im.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge in sparse or not column\n",
    "im['is_sparse'] = 0\n",
    "\n",
    "im.loc[(im[item_id].isin(list(sparse[item_id].unique()))), 'is_sparse'] = 1\n",
    "print(im.is_sparse.value_counts(dropna=False))\n",
    "im.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge in top-moving or not column\n",
    "im['top_moving'] = 0\n",
    "\n",
    "im.loc[(im[item_id].isin(list(top_moving_items[item_id].unique()))), 'top_moving'] = 1\n",
    "print(im.top_moving.value_counts(dropna=False))\n",
    "im.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge in time series categories column\n",
    "categories_df = full_history.groupby([item_id])[item_id, 'ts_type'].first()\n",
    "categories_df.reset_index(inplace=True, drop=True)\n",
    "# categories_df.head(2)\n",
    "\n",
    "im = im.merge(categories_df, how=\"left\", on=[item_id])\n",
    "print(im.ts_type.value_counts(dropna=False))\n",
    "im.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assemble metadata\n",
    "\n",
    "im = im.groupby(item_id).max()\n",
    "im.reset_index(inplace=True)\n",
    "print(im.shape)\n",
    "print(\"checking nulls..\")\n",
    "print(im.isnull().sum())\n",
    "im.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save im to S3\n",
    "local_file = \"tts.csv\"\n",
    "# Save merged file locally\n",
    "im.to_csv(local_file, header=False, index=False)\n",
    "\n",
    "key = f\"{prefix}/v{DATA_VERSION}/{EXPERIMENT_NAME}.metadata.csv\"\n",
    "boto3.Session().resource('s3').Bucket(bucket_name).Object(key).upload_file(local_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
